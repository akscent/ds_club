{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install pymorphy2 cleantext -U nlp_profiler textblob pymystem3\nimport os\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nsys.path.insert(1, '/kaggle/input/ods-huawei/nlp_huawei_new2_task-master/nlp_huawei_new2_task-master/baseline_transformers')\n# from dataset import *\n# from model import *\n# from trainer import Trainer\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom typing import Dict\nimport json\nfrom numpy import asarray\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam, AdamW\nfrom tqdm.notebook import tqdm\nfrom textblob import TextBlob\n\ntorch.manual_seed(42)","metadata":{"jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2023-11-05T09:55:44.189511Z","iopub.execute_input":"2023-11-05T09:55:44.189920Z","iopub.status.idle":"2023-11-05T09:55:44.203315Z","shell.execute_reply.started":"2023-11-05T09:55:44.189881Z","shell.execute_reply":"2023-11-05T09:55:44.202059Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7ae1709ead10>"},"metadata":{}}]},{"cell_type":"code","source":"class FiveDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_seq_len):\n        self.data = dataframe\n        self.text = dataframe['text'].tolist()\n        self.targets = None\n        if 'rate' in dataframe:\n            self.targets = dataframe['rate'].tolist()\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = ' '.join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_seq_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True,\n            truncation=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        if self.targets is not None:\n            return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n                'targets': torch.tensor(self.targets[index], dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n            }\n\n    def __len__(self) -> int:\n        return len(self.text)\n    \n\nclass ModelForClassification(torch.nn.Module):\n\n    def __init__(self, model_path: str, config: Dict):\n        super(ModelForClassification, self).__init__()\n        self.model_name = model_path\n        self.config = config\n        self.n_classes = config['num_classes']\n        self.dropout_rate = config['dropout_rate']\n        self.bert = AutoModel.from_pretrained(self.model_name)\n        self.pre_classifier = torch.nn.Linear(312, 768)\n        self.dropout = torch.nn.Dropout(self.dropout_rate)\n        self.classifier = torch.nn.Linear(768, self.n_classes)\n        self.softmax = torch.nn.LogSoftmax(dim = 1)\n\n    def forward(self, input_ids, attention_mask,):\n        output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        hidden_state = output[0]\n        hidden_state = hidden_state[:, 0]\n        hidden_state = self.pre_classifier(hidden_state)\n        hidden_state = torch.nn.ReLU()(hidden_state)\n        hidden_state = self.dropout(hidden_state)\n        output = self.classifier(hidden_state)\n        output = self.softmax(output)\n        return output\n\n\nclass Trainer:\n    def __init__(self, config: Dict, class_weights=None):\n        self.config = config\n        self.device = config['device']\n        self.n_epochs = config['n_epochs']\n        self.optimizer = None\n        self.opt_fn = lambda model: AdamW(model.parameters(), config['lr'])\n        self.model = None\n        self.history = None\n        if class_weights is not None:\n            class_weights = class_weights.to(self.device)\n            self.loss_fn = CrossEntropyLoss(weight=class_weights)\n        else:\n            self.loss_fn = CrossEntropyLoss()\n        self.device = config['device']\n        self.verbose = config.get('verbose', True)\n        \n    def save_history(self, path: str):\n        history = {\n            'train_loss': self.history['train_loss'],\n            'val_loss': self.history['val_loss'],\n            'val_acc': self.history['val_acc']\n        }\n        val_acc = sum(self.history['val_acc']) / len(self.history['val_acc'])\n        print(\"All ACCURACY = \", val_acc)\n        with open(path, 'w') as file:\n            json.dump(history, file)\n        \n    def load_history(self, path: str):\n        with open(path, 'r') as file:\n            history = json.load(file)\n        self.history = {\n            'train_loss': history['train_loss'],\n            'val_loss': history['val_loss'],\n            'val_acc': history['val_acc']\n        }\n\n    def fit(self, model, train_dataloader, val_dataloader):\n        self.model = model.to(self.device)\n        self.optimizer = self.opt_fn(model)\n        self.history = {\n            'train_loss': [],\n            'val_loss': [],\n            'val_acc': []\n        }\n        best_val_loss = float('inf')\n\n        for epoch in range(self.n_epochs):\n            print(f\"Epoch {epoch + 1}/{self.n_epochs}\")\n            train_info = self.train_epoch(train_dataloader)\n            val_info = self.val_epoch(val_dataloader)\n            self.history['train_loss'].extend(train_info['loss'])\n            self.history['val_loss'].extend([val_info['loss']])\n            self.history['val_acc'].extend([val_info['acc']])\n\n            if val_info['loss'] < best_val_loss:\n                best_val_loss = val_info['loss']\n                self.save_model_weights('best_model_weights.ckpt')\n\n            self.save_history('history.json')\n\n        return self.model.eval()\n\n    def save_model_weights(self, path: str):\n        torch.save(self.model.state_dict(), path)\n\n\n\n    def train_epoch(self, train_dataloader):\n        self.model.train()\n        losses = []\n        total_loss = 0\n        if self.verbose:\n            train_dataloader = tqdm(train_dataloader)\n        for batch in train_dataloader:\n            ids = batch['ids'].to(self.device, dtype=torch.long)\n            mask = batch['mask'].to(self.device, dtype=torch.long)\n            targets = batch['targets'].to(self.device, dtype=torch.long)\n\n            outputs = self.model(ids, mask)\n            loss = self.loss_fn(outputs, targets)\n            total_loss += loss.item()\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            loss_val = loss.item()\n            if self.verbose:\n                train_dataloader.set_description(f\"Loss={loss_val:.3}\")\n            losses.append(loss_val)\n        avg_loss = total_loss / len(train_dataloader)\n        print(\"AVG LOSS = \", avg_loss)\n        return {'loss': losses}\n\n    def val_epoch(self, val_dataloader):\n        self.model.eval()\n        all_logits = []\n        all_labels = []\n        if self.verbose:\n            val_dataloader = tqdm(val_dataloader)\n        with torch.no_grad():\n            for batch in val_dataloader:\n                ids = batch['ids'].to(self.device, dtype=torch.long)\n                mask = batch['mask'].to(self.device, dtype=torch.long)\n                targets = batch['targets'].to(self.device, dtype=torch.long)\n                outputs = self.model(ids, mask)\n                all_logits.append(outputs)\n                all_labels.append(targets)\n        all_labels = torch.cat(all_labels).to(self.device)\n        all_logits = torch.cat(all_logits).to(self.device)\n        loss = self.loss_fn(all_logits, all_labels).item()\n        acc = (all_logits.argmax(1) == all_labels).float().mean().item()\n        print(\"ACCURACY for EPOCH = \", acc)\n        if self.verbose:\n            val_dataloader.set_description(f\"Loss={loss:.3}; Acc:{acc:.3}\")\n        return {\n            'acc': acc,\n            'loss': loss\n        }\n\n    def predict(self, test_dataloader):\n        if not self.model:\n            raise RuntimeError(\"You should train the model first\")\n        self.model.eval()\n        predictions = []\n        with torch.no_grad():\n            for batch in test_dataloader:\n                ids = batch['ids'].to(self.device, dtype=torch.long)\n                mask = batch['mask'].to(self.device, dtype=torch.long)\n                outputs = self.model(ids, mask)\n                preds = torch.exp(outputs)\n                predictions.extend(preds.tolist())\n        return asarray(predictions)\n\n    def save(self, path: str):\n        if self.model is None:\n            raise RuntimeError(\"You should train the model first\")\n        checkpoint = {\n            \"config\": self.model.config,\n            \"trainer_config\": self.config,\n            \"model_name\": self.model.model_name,\n            \"model_state_dict\": self.model.state_dict()\n        }\n        torch.save(checkpoint, path)\n\n    def plot_history(self):\n        import matplotlib.pyplot as plt\n        \n        if self.history is None:\n            raise RuntimeError(\"History is not available. Train the model first.\")\n\n        train_loss = self.history['train_loss']\n        val_loss = self.history['val_loss']\n        val_acc = self.history['val_acc']\n\n        epochs = range(1, len(train_loss) + 1)\n\n        plt.figure(figsize=(12, 5))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, train_loss, 'bo', label='Training loss')\n        plt.plot(epochs, val_loss, 'r', label='Validation loss')\n        plt.title('Training and Validation Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, val_acc, 'g', label='Validation accuracy')\n        plt.title('Validation Accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n\n        plt.show()\n\n\n    @classmethod\n    def load(cls, path: str):\n        ckpt = torch.load(path)\n        keys = [\"config\", \"trainer_config\", \"model_state_dict\"]\n        for key in keys:\n            if key not in ckpt:\n                raise RuntimeError(f\"Missing key {key} in checkpoint\")\n        new_model = ModelForClassification(\n            ckpt['model_name'],\n            ckpt[\"config\"]\n        )\n        new_model.load_state_dict(ckpt[\"model_state_dict\"])\n        new_trainer = cls(ckpt[\"trainer_config\"])\n        new_trainer.model = new_model\n        new_trainer.model.to(new_trainer.device)\n        return new_trainer","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:55:55.104441Z","iopub.execute_input":"2023-11-05T09:55:55.104833Z","iopub.status.idle":"2023-11-05T09:55:55.159263Z","shell.execute_reply.started":"2023-11-05T09:55:55.104802Z","shell.execute_reply":"2023-11-05T09:55:55.158433Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data load","metadata":{}},{"cell_type":"code","source":"PATH = \"/kaggle/input/ods-huawei/\"\ntrain_data = pd.read_csv(os.path.join(PATH, \"train.csv\"))\ntest_data = pd.read_csv(os.path.join(PATH, \"test.csv\"))\nle = LabelEncoder()\ntrain_data.rate = le.fit_transform(train_data.rate)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:33:00.589473Z","iopub.execute_input":"2023-11-05T09:33:00.589881Z","iopub.status.idle":"2023-11-05T09:33:01.195525Z","shell.execute_reply.started":"2023-11-05T09:33:00.589850Z","shell.execute_reply":"2023-11-05T09:33:01.194448Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   rate                                               text\n0     3  Очень понравилось. Были в начале марта  с соба...\n1     4  В целом магазин устраивает.\\nАссортимент позво...\n2     4  Очень хорошо что открылась 5 ка, теперь не над...\n3     2  Пятёрочка громко объявила о том как она заботи...\n4     2  Тесно, вечная сутолока, между рядами трудно ра...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rate</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Очень понравилось. Были в начале марта  с соба...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>В целом магазин устраивает.\\nАссортимент позво...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Очень хорошо что открылась 5 ка, теперь не над...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Пятёрочка громко объявила о том как она заботи...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Тесно, вечная сутолока, между рядами трудно ра...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Pre-clean text","metadata":{}},{"cell_type":"code","source":"# import re\n# TOKEN_RE = re.compile(r'[а-яё]+')\n\n# def tokenize_text(text, min_length_token=1):\n#     text = text.lower()\n#     tokens = TOKEN_RE.findall(text)\n#     return [token for token in tokens if len(token) >= min_length_token]\n\n# def text_cleaning(text):\n#     tokens = tokenize_text(text)\n#     return ' '.join(tokens)\n\n# tqdm.pandas()\n# train_data['text'] = train_data['text'].progress_apply(text_cleaning)\n# test_data['text'] = test_data['text'].progress_apply(text_cleaning)\n\nimport re\nimport pymorphy2\n\nfrom nltk.corpus import stopwords\n\nru_stopwords = stopwords.words('russian')\ndigits = [str(i) for i in range(10)]\n\nTOKEN_RE = re.compile(r'[а-яё!.,?%]+')\nlemmatizer = pymorphy2.MorphAnalyzer()\n\ndef is_valid_word(word):\n    if not word[0].isdigit() and word not in ru_stopwords:\n        parsed_word = lemmatizer.normal_forms(word)[0]\n        return parsed_word\n    return False\n\ndef text_cleaning(text):\n    text = re.sub(r'[^a-zA-Zа-яА-Я0-9\\s.,!?]', '', text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    words = text.split()\n    cleaned_words = [word for word in words[:512] if is_valid_word(word) and len(word) < 15]\n    cleaned_text = ' '.join(cleaned_words)\n    return cleaned_text\n\ntqdm.pandas()\ntrain_data['text'] = train_data['text'].progress_apply(text_cleaning)\ntest_data['text'] = test_data['text'].progress_apply(text_cleaning)\n\ntrain_data[\"num_words\"] = train_data[\"text\"].apply(\n    lambda x: len(str(x).split()))\ntest_data[\"num_words\"] = test_data[\"text\"].apply(\n    lambda x: len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:25:20.856457Z","iopub.execute_input":"2023-11-02T12:25:20.856873Z","iopub.status.idle":"2023-11-02T12:25:21.085304Z","shell.execute_reply.started":"2023-11-02T12:25:20.856845Z","shell.execute_reply":"2023-11-02T12:25:21.08449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data.to_csv(\"cleaned_train.csv\", index=False)\n# test_data.to_csv(\"cleaned_test.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T09:52:23.592141Z","iopub.execute_input":"2023-11-02T09:52:23.593014Z","iopub.status.idle":"2023-11-02T09:52:24.013543Z","shell.execute_reply.started":"2023-11-02T09:52:23.592981Z","shell.execute_reply":"2023-11-02T09:52:24.012516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del zero\ntrain_data = train_data[train_data['num_words'] != 0]\ntest_data = test_data[test_data['num_words'] != 0]","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:28:09.545003Z","iopub.execute_input":"2023-11-02T12:28:09.545388Z","iopub.status.idle":"2023-11-02T12:28:09.563748Z","shell.execute_reply.started":"2023-11-02T12:28:09.54536Z","shell.execute_reply":"2023-11-02T12:28:09.562987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\ndef remove_infrequent_words(dataset, min_count=3):\n    word_counter = Counter()\n    for text in dataset:\n        words = text.split()\n        word_counter.update(words)\n    infrequent_words = [word for word, count in word_counter.items() if count < min_count]\n    def remove_infrequent(text):\n        words = text.split()\n        cleaned_words = [word for word in words if word not in infrequent_words]\n        cleaned_text = ' '.join(cleaned_words)\n        return cleaned_text\n    cleaned_dataset = [remove_infrequent(text) for text in tqdm(dataset, desc=\"Cleaning text\")]\n\n    return cleaned_dataset\n\ncleaned_train = remove_infrequent_words(train_data['text'].tolist())\ncleaned_test = remove_infrequent_words(test_data['text'].tolist())\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T10:47:40.039711Z","iopub.execute_input":"2023-11-02T10:47:40.039972Z","iopub.status.idle":"2023-11-02T10:58:36.064401Z","shell.execute_reply.started":"2023-11-02T10:47:40.039949Z","shell.execute_reply":"2023-11-02T10:58:36.063384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['cleaned_text'] = cleaned_train\ntest_data['cleaned_text'] = cleaned_test\ntrain_data.to_csv(\"cleaned_train.csv\", index=False)\ntest_data.to_csv(\"cleaned_test.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T10:58:36.066444Z","iopub.execute_input":"2023-11-02T10:58:36.066743Z","iopub.status.idle":"2023-11-02T10:58:36.783497Z","shell.execute_reply.started":"2023-11-02T10:58:36.066717Z","shell.execute_reply":"2023-11-02T10:58:36.782435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# New data","metadata":{}},{"cell_type":"code","source":"PATH = \"/kaggle/input/cleaned-text\"\ntrain_data = pd.read_csv(os.path.join(PATH, \"cleaned_train (1).csv\"))\ntest_data = pd.read_csv(os.path.join(PATH, \"cleaned_test (1).csv\"))\n# del zero\ntrain_data = train_data[train_data['num_words'] != 0]\ntest_data = test_data[test_data['num_words'] != 0]\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:20:32.788293Z","iopub.execute_input":"2023-11-02T12:20:32.789167Z","iopub.status.idle":"2023-11-02T12:20:33.441836Z","shell.execute_reply.started":"2023-11-02T12:20:32.789137Z","shell.execute_reply":"2023-11-02T12:20:33.440896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace nan\n\ndef replace_nan_with_text(row):\n    if pd.isna(row['cleaned_text']):\n        return row['text']\n    return row['cleaned_text']\n\ntrain_data['cleaned_text'] = train_data.progress_apply(replace_nan_with_text, axis=1)\ntest_data['cleaned_text'] = test_data.progress_apply(replace_nan_with_text, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:36:52.289989Z","iopub.execute_input":"2023-11-02T12:36:52.290379Z","iopub.status.idle":"2023-11-02T12:36:53.36383Z","shell.execute_reply.started":"2023-11-02T12:36:52.290353Z","shell.execute_reply":"2023-11-02T12:36:53.362988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def truncate_text(text, max_words=512):\n    words = text.split()\n    if len(words) > max_words:\n        truncated_text = ' '.join(words[:max_words])\n    else:\n        truncated_text = text\n    return truncated_text\n\ntqdm.pandas()\ntrain_data['cleaned_text'] = train_data['cleaned_text'].progress_apply(truncate_text)\ntest_data['cleaned_text'] = test_data['cleaned_text'].progress_apply(truncate_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:37:06.600624Z","iopub.execute_input":"2023-11-02T12:37:06.601315Z","iopub.status.idle":"2023-11-02T12:37:06.854512Z","shell.execute_reply.started":"2023-11-02T12:37:06.601282Z","shell.execute_reply":"2023-11-02T12:37:06.853634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# идея суммирования текста в более короткий текст\n\n\nfrom transformers import MBartTokenizer, MBartForConditionalGeneration\n\nmodel_name = \"IlyaGusev/mbart_ru_sum_gazeta\"\ntokenizer = MBartTokenizer.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\n\ndef summary_rows(article_text):\n    input_ids = tokenizer(\n        [article_text],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    )[\"input_ids\"]\n\n    output_ids = model.generate(\n        input_ids=input_ids,\n        no_repeat_ngram_size=4\n    )[0]\n\n    summary = tokenizer.decode(output_ids, skip_special_tokens=True)\n    return summary\n\ndef text_summary(text):\n    if isinstance(text, str) and text.strip() and len(str(text).split()) > 150:\n        return summary_rows(text)\n    else:\n        return text\n    \n\ntrain_data['summary'] = train_data['cleaned_text'].progress_apply(text_summary)\ntest_data['summary'] = test_data['cleaned_text'].progress_apply(text_summary)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:06:53.929167Z","iopub.execute_input":"2023-11-02T13:06:53.929998Z","iopub.status.idle":"2023-11-02T13:12:13.695153Z","shell.execute_reply.started":"2023-11-02T13:06:53.929963Z","shell.execute_reply":"2023-11-02T13:12:13.694172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:16:56.244022Z","iopub.execute_input":"2023-11-02T13:16:56.244703Z","iopub.status.idle":"2023-11-02T13:16:56.261445Z","shell.execute_reply.started":"2023-11-02T13:16:56.24467Z","shell.execute_reply":"2023-11-02T13:16:56.260356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"apanc/russian-sensitive-topics\")\ntokenizer = AutoTokenizer.from_pretrained(\"apanc/russian-sensitive-topics\")\ntokenizer.padding = True\ntokenizer.truncation = True\ntokenizer.max_length = 512\npipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=torch.device(\"cuda:0\"))\n\ndef make_pipe(text):\n    return pipe(text, return_all_scores=True)\n\ntqdm.pandas()\ntrain_data['theme_labels'] = train_data['summary'].progress_apply(make_pipe)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:34:13.566451Z","iopub.execute_input":"2023-11-02T13:34:13.566892Z","iopub.status.idle":"2023-11-02T13:43:27.920989Z","shell.execute_reply.started":"2023-11-02T13:34:13.566858Z","shell.execute_reply":"2023-11-02T13:43:27.919894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_label_probs(row):\n    label_probs = [label['score'] for label in row[0]]\n    return label_probs\n\ntrain_data['label_probs'] = train_data['theme_labels'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].apply(pd.Series).add_prefix('LABEL_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['theme_labels']","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:51:08.264219Z","iopub.execute_input":"2023-11-02T13:51:08.265013Z","iopub.status.idle":"2023-11-02T13:51:21.477698Z","shell.execute_reply.started":"2023-11-02T13:51:08.264981Z","shell.execute_reply":"2023-11-02T13:51:21.476723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature data\ntrain_data.to_csv(\"feature_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:55:00.859369Z","iopub.execute_input":"2023-11-02T13:55:00.860058Z","iopub.status.idle":"2023-11-02T13:55:42.22507Z","shell.execute_reply.started":"2023-11-02T13:55:00.860024Z","shell.execute_reply":"2023-11-02T13:55:42.223909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # добавление переменных о чувствах\n# from transformers import BertTokenizer, BertForSequenceClassification\n# model_name = 'Skoltech/russian-sensitive-topics'\n# tokenizer = BertTokenizer.from_pretrained(model_name)\n# model = BertForSequenceClassification.from_pretrained(model_name);\n\n# tokenized = tokenizer.batch_encode_plus(train_data[train_data[\"num_words\"] > 80]['text'][370],\n#                                         max_length = 512,\n#                                         pad_to_max_length=True,\n#                                         truncation=True,\n#                                         return_token_type_ids=False)\n\n# tokens_ids,mask = torch.tensor(tokenized['input_ids']),torch.tensor(tokenized['attention_mask']) \n\n# with torch.no_grad():\n#     model_output = model(tokens_ids,mask)\n\n# def adjust_multilabel(y, is_pred = False):\n#     y_adjusted = []\n#     for y_c in y:\n#         y_test_curr = [0]*19\n#         index = str(int(np.argmax(y_c)))\n#         y_c = target_vaiables_id2topic_dict[index]\n#     return y_c\n\n# model_output","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:36:45.408569Z","iopub.execute_input":"2023-11-02T06:36:45.408967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# тональность текста\npipe = pipeline(model=\"seara/rubert-tiny2-russian-sentiment\", device=torch.device(\"cuda:0\"))\n\ntqdm.pandas()\ntrain_data['mood'] = train_data['summary'].progress_apply(make_pipe)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:59:16.176219Z","iopub.execute_input":"2023-11-02T13:59:16.176629Z","iopub.status.idle":"2023-11-02T14:02:10.686238Z","shell.execute_reply.started":"2023-11-02T13:59:16.176598Z","shell.execute_reply":"2023-11-02T14:02:10.685167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['label_probs'] = train_data['mood'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('MOOD_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['mood']","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:03:17.818576Z","iopub.execute_input":"2023-11-02T14:03:17.819225Z","iopub.status.idle":"2023-11-02T14:03:26.971199Z","shell.execute_reply.started":"2023-11-02T14:03:17.819194Z","shell.execute_reply":"2023-11-02T14:03:26.970203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature data\ntrain_data.to_csv(\"feature_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:03:34.684386Z","iopub.execute_input":"2023-11-02T14:03:34.68514Z","iopub.status.idle":"2023-11-02T14:03:34.733046Z","shell.execute_reply.started":"2023-11-02T14:03:34.685108Z","shell.execute_reply":"2023-11-02T14:03:34.732163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# токичность\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# tokenizer = BertTokenizer.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')\n# model = BertForSequenceClassification.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')\n# batch = tokenizer.encode(train_data[train_data[\"num_words\"] > 80]['text'][48421], return_tensors='pt')\n# model(batch)\n\npipe = pipeline(model=\"SkolkovoInstitute/russian_toxicity_classifier\", device=torch.device(\"cuda:0\"))\n\ntqdm.pandas()\ntrain_data['toxic'] = train_data['summary'].progress_apply(make_pipe)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:06:29.607208Z","iopub.execute_input":"2023-11-02T14:06:29.607658Z","iopub.status.idle":"2023-11-02T14:15:18.766236Z","shell.execute_reply.started":"2023-11-02T14:06:29.607627Z","shell.execute_reply":"2023-11-02T14:15:18.765249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['label_probs'] = train_data['toxic'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('TOXIC_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['toxic']","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:17:22.553177Z","iopub.execute_input":"2023-11-02T14:17:22.553938Z","iopub.status.idle":"2023-11-02T14:17:31.896653Z","shell.execute_reply.started":"2023-11-02T14:17:22.553904Z","shell.execute_reply":"2023-11-02T14:17:31.895878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# эмоции\nimport torch\nfrom transformers import BertForSequenceClassification, AutoTokenizer\n\nLABELS = ['neutral', 'happiness', 'sadness', 'enthusiasm', 'fear', 'anger', 'disgust']\ntokenizer = AutoTokenizer.from_pretrained('Aniemore/rubert-tiny2-russian-emotion-detection')\nmodel = BertForSequenceClassification.from_pretrained('Aniemore/rubert-tiny2-russian-emotion-detection')\n\n@torch.no_grad()\ndef predict_emotion(text: str) -> str:\n    \"\"\"\n        We take the input text, tokenize it, pass it through the model, and then return the predicted label\n        :param text: The text to be classified\n        :type text: str\n        :return: The predicted emotion\n    \"\"\"\n    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n    outputs = model(**inputs)\n    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n    predicted = torch.argmax(predicted, dim=1).numpy()\n        \n    return LABELS[predicted[0]]\n\n@torch.no_grad()    \ndef predict_emotions(text: str) -> list:\n    \"\"\"\n        It takes a string of text, tokenizes it, feeds it to the model, and returns a dictionary of emotions and their\n        probabilities\n        :param text: The text you want to classify\n        :type text: str\n        :return: A dictionary of emotions and their probabilities.\n    \"\"\"\n    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n    outputs = model(**inputs)\n    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n    emotions_list = {}\n    for i in range(len(predicted.numpy()[0].tolist())):\n        emotions_list[LABELS[i]] = predicted.numpy()[0].tolist()[i]\n    return emotions_list\n\ntrain_data['toxic'] = train_data['summary'].progress_apply(predict_emotions)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:20:12.685216Z","iopub.execute_input":"2023-11-02T14:20:12.685599Z","iopub.status.idle":"2023-11-02T14:24:57.905123Z","shell.execute_reply.started":"2023-11-02T14:20:12.685571Z","shell.execute_reply":"2023-11-02T14:24:57.904146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['toxic'][0]","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:31:52.240886Z","iopub.execute_input":"2023-11-02T14:31:52.241277Z","iopub.status.idle":"2023-11-02T14:31:52.248065Z","shell.execute_reply.started":"2023-11-02T14:31:52.241248Z","shell.execute_reply":"2023-11-02T14:31:52.247085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_label_probs(row):\n    label_probs = [row.get(label, 0.0) for label in LABELS]\n    return label_probs\n\ntrain_data['label_probs'] = train_data['toxic'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('EMOTION_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['toxic']","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:31:54.475101Z","iopub.execute_input":"2023-11-02T14:31:54.47577Z","iopub.status.idle":"2023-11-02T14:32:03.918764Z","shell.execute_reply.started":"2023-11-02T14:31:54.475737Z","shell.execute_reply":"2023-11-02T14:32:03.917974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature data\ntrain_data.to_csv(\"feature_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T14:33:04.238436Z","iopub.execute_input":"2023-11-02T14:33:04.239379Z","iopub.status.idle":"2023-11-02T14:33:48.126658Z","shell.execute_reply.started":"2023-11-02T14:33:04.239345Z","shell.execute_reply":"2023-11-02T14:33:48.125516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# the end poka","metadata":{}},{"cell_type":"code","source":"# textblob - обработка текста, генерация фич https://textblob.readthedocs.io/en/dev/quickstart.html\n# еще одна библиотека для классификации текстов https://small-text.readthedocs.io/en/latest/\n# полярность слов https://polyglot.readthedocs.io/en/latest/\n# обработка фич https://github.com/jbesomi/texthero\n# фичегенерация https://github.com/neomatrix369/nlp_profiler#Notebooks\n# классификация на других предобученных моделях, перечисленных у Алерона https://github.com/a-milenkin/Competitive_Data_Science/blob/main/notebooks/9.2.1%20-%20Text_Embeddings.ipynb\n# использовать эти ноутбуки для классификации https://github.com/e0xextazy/vkcup2022-first-stage/blob/main/inference.ipynb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"/kaggle/input/ods-huawei/\"\ntrain_data = pd.read_csv(os.path.join(PATH, \"feature_train.csv\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:34:06.007444Z","iopub.execute_input":"2023-11-05T09:34:06.007823Z","iopub.status.idle":"2023-11-05T09:34:18.899219Z","shell.execute_reply.started":"2023-11-05T09:34:06.007794Z","shell.execute_reply":"2023-11-05T09:34:18.898113Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data[\"num_words\"] = train_data[\"text\"].apply(\n    lambda x: len(str(x).split()))\ntest_data[\"num_words\"] = test_data[\"text\"].apply(\n    lambda x: len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:34:18.901340Z","iopub.execute_input":"2023-11-05T09:34:18.901701Z","iopub.status.idle":"2023-11-05T09:34:19.062125Z","shell.execute_reply.started":"2023-11-05T09:34:18.901669Z","shell.execute_reply":"2023-11-05T09:34:19.061045Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:34:52.546575Z","iopub.execute_input":"2023-11-05T09:34:52.546991Z","iopub.status.idle":"2023-11-05T09:34:52.583190Z","shell.execute_reply.started":"2023-11-05T09:34:52.546955Z","shell.execute_reply":"2023-11-05T09:34:52.582011Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   rate                                               text  num_words  \\\n0     3  Очень понравилось. Были начале марта собакой. ...         29   \n1     4  В целом магазин устраивает. Ассортимент позвол...         39   \n2     4            Очень открылась ка, далеко ехать рядом!          6   \n3     2  Пятрочка громко объявила заботится пенсионерах...         26   \n4     2  Тесно, вечная сутолока, рядами трудно разойтис...         12   \n5     3  Магазин пешей доступности. После ремонта рекон...         13   \n6     4  Магазин хороший цены скидки нормальные токо вр...         13   \n7     2  Редко сюда забегаю. Маленький магазинчик, это ...         22   \n8     4       Сложно найти торговом центре. А магазин норм          7   \n9     3  После ремонта магазин нутри стал ещ лучше. Бол...         18   \n\n                                        cleaned_text  \\\n0  Очень понравилось. Были начале марта собакой. ...   \n1  В целом магазин устраивает. Ассортимент позвол...   \n2            Очень открылась ка, далеко ехать рядом!   \n3  Пятрочка громко заботится часы посещения магаз...   \n4  Тесно, вечная рядами трудно разойтись, грязно....   \n5  Магазин пешей доступности. После ремонта рекон...   \n6  Магазин хороший цены скидки нормальные токо вр...   \n7  Редко сюда забегаю. Маленький магазинчик, это ...   \n8       Сложно найти торговом центре. А магазин норм   \n9  После ремонта магазин нутри стал ещ лучше. Бол...   \n\n                                             summary   LABEL_0   LABEL_1  \\\n0  Очень понравилось. Были начале марта собакой. ...  0.996648  0.000086   \n1  В целом магазин устраивает. Ассортимент позвол...  0.997240  0.000076   \n2            Очень открылась ка, далеко ехать рядом!  0.982682  0.000592   \n3  Пятрочка громко заботится часы посещения магаз...  0.391624  0.005887   \n4  Тесно, вечная рядами трудно разойтись, грязно....  0.996155  0.000161   \n5  Магазин пешей доступности. После ремонта рекон...  0.769079  0.001755   \n6  Магазин хороший цены скидки нормальные токо вр...  0.994967  0.000159   \n7  Редко сюда забегаю. Маленький магазинчик, это ...  0.948751  0.000679   \n8       Сложно найти торговом центре. А магазин норм  0.565123  0.006904   \n9  После ремонта магазин нутри стал ещ лучше. Бол...  0.993944  0.000154   \n\n    LABEL_2   LABEL_3   LABEL_4  ...    MOOD_2   TOXIC_0   TOXIC_1  EMOTION_0  \\\n0  0.000343  0.000294  0.000066  ...  0.003136  0.998674  0.001326   0.000406   \n1  0.000275  0.000140  0.000051  ...  0.075873  0.998687  0.001313   0.999554   \n2  0.001711  0.001077  0.000292  ...  0.115785  0.997499  0.002501   0.001433   \n3  0.000527  0.003289  0.001155  ...  0.199057  0.996688  0.003312   0.999452   \n4  0.000396  0.000172  0.000082  ...  0.429277  0.995250  0.004750   0.000524   \n5  0.000365  0.002494  0.001035  ...  0.004294  0.998610  0.001390   0.999216   \n6  0.001130  0.000186  0.000240  ...  0.028344  0.998878  0.001122   0.999532   \n7  0.000612  0.019587  0.001260  ...  0.232759  0.995381  0.004619   0.998663   \n8  0.000616  0.007276  0.004125  ...  0.053535  0.998884  0.001117   0.999540   \n9  0.000255  0.000182  0.000107  ...  0.120662  0.999014  0.000986   0.999063   \n\n   EMOTION_1  EMOTION_2  EMOTION_3  EMOTION_4  EMOTION_5  EMOTION_6  \n0   0.998069   0.000312   0.000484   0.000268   0.000328   0.000132  \n1   0.000076   0.000105   0.000081   0.000069   0.000072   0.000043  \n2   0.941779   0.050075   0.001406   0.002641   0.001579   0.001087  \n3   0.000091   0.000123   0.000112   0.000076   0.000097   0.000049  \n4   0.000424   0.996605   0.000305   0.000807   0.000924   0.000411  \n5   0.000230   0.000144   0.000119   0.000093   0.000132   0.000066  \n6   0.000100   0.000090   0.000086   0.000067   0.000078   0.000047  \n7   0.000173   0.000143   0.000500   0.000154   0.000271   0.000096  \n8   0.000078   0.000102   0.000090   0.000073   0.000074   0.000043  \n9   0.000261   0.000107   0.000220   0.000092   0.000182   0.000075  \n\n[10 rows x 410 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rate</th>\n      <th>text</th>\n      <th>num_words</th>\n      <th>cleaned_text</th>\n      <th>summary</th>\n      <th>LABEL_0</th>\n      <th>LABEL_1</th>\n      <th>LABEL_2</th>\n      <th>LABEL_3</th>\n      <th>LABEL_4</th>\n      <th>...</th>\n      <th>MOOD_2</th>\n      <th>TOXIC_0</th>\n      <th>TOXIC_1</th>\n      <th>EMOTION_0</th>\n      <th>EMOTION_1</th>\n      <th>EMOTION_2</th>\n      <th>EMOTION_3</th>\n      <th>EMOTION_4</th>\n      <th>EMOTION_5</th>\n      <th>EMOTION_6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n      <td>29</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n      <td>0.996648</td>\n      <td>0.000086</td>\n      <td>0.000343</td>\n      <td>0.000294</td>\n      <td>0.000066</td>\n      <td>...</td>\n      <td>0.003136</td>\n      <td>0.998674</td>\n      <td>0.001326</td>\n      <td>0.000406</td>\n      <td>0.998069</td>\n      <td>0.000312</td>\n      <td>0.000484</td>\n      <td>0.000268</td>\n      <td>0.000328</td>\n      <td>0.000132</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n      <td>39</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n      <td>0.997240</td>\n      <td>0.000076</td>\n      <td>0.000275</td>\n      <td>0.000140</td>\n      <td>0.000051</td>\n      <td>...</td>\n      <td>0.075873</td>\n      <td>0.998687</td>\n      <td>0.001313</td>\n      <td>0.999554</td>\n      <td>0.000076</td>\n      <td>0.000105</td>\n      <td>0.000081</td>\n      <td>0.000069</td>\n      <td>0.000072</td>\n      <td>0.000043</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n      <td>6</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n      <td>0.982682</td>\n      <td>0.000592</td>\n      <td>0.001711</td>\n      <td>0.001077</td>\n      <td>0.000292</td>\n      <td>...</td>\n      <td>0.115785</td>\n      <td>0.997499</td>\n      <td>0.002501</td>\n      <td>0.001433</td>\n      <td>0.941779</td>\n      <td>0.050075</td>\n      <td>0.001406</td>\n      <td>0.002641</td>\n      <td>0.001579</td>\n      <td>0.001087</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Пятрочка громко объявила заботится пенсионерах...</td>\n      <td>26</td>\n      <td>Пятрочка громко заботится часы посещения магаз...</td>\n      <td>Пятрочка громко заботится часы посещения магаз...</td>\n      <td>0.391624</td>\n      <td>0.005887</td>\n      <td>0.000527</td>\n      <td>0.003289</td>\n      <td>0.001155</td>\n      <td>...</td>\n      <td>0.199057</td>\n      <td>0.996688</td>\n      <td>0.003312</td>\n      <td>0.999452</td>\n      <td>0.000091</td>\n      <td>0.000123</td>\n      <td>0.000112</td>\n      <td>0.000076</td>\n      <td>0.000097</td>\n      <td>0.000049</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Тесно, вечная сутолока, рядами трудно разойтис...</td>\n      <td>12</td>\n      <td>Тесно, вечная рядами трудно разойтись, грязно....</td>\n      <td>Тесно, вечная рядами трудно разойтись, грязно....</td>\n      <td>0.996155</td>\n      <td>0.000161</td>\n      <td>0.000396</td>\n      <td>0.000172</td>\n      <td>0.000082</td>\n      <td>...</td>\n      <td>0.429277</td>\n      <td>0.995250</td>\n      <td>0.004750</td>\n      <td>0.000524</td>\n      <td>0.000424</td>\n      <td>0.996605</td>\n      <td>0.000305</td>\n      <td>0.000807</td>\n      <td>0.000924</td>\n      <td>0.000411</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>Магазин пешей доступности. После ремонта рекон...</td>\n      <td>13</td>\n      <td>Магазин пешей доступности. После ремонта рекон...</td>\n      <td>Магазин пешей доступности. После ремонта рекон...</td>\n      <td>0.769079</td>\n      <td>0.001755</td>\n      <td>0.000365</td>\n      <td>0.002494</td>\n      <td>0.001035</td>\n      <td>...</td>\n      <td>0.004294</td>\n      <td>0.998610</td>\n      <td>0.001390</td>\n      <td>0.999216</td>\n      <td>0.000230</td>\n      <td>0.000144</td>\n      <td>0.000119</td>\n      <td>0.000093</td>\n      <td>0.000132</td>\n      <td>0.000066</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4</td>\n      <td>Магазин хороший цены скидки нормальные токо вр...</td>\n      <td>13</td>\n      <td>Магазин хороший цены скидки нормальные токо вр...</td>\n      <td>Магазин хороший цены скидки нормальные токо вр...</td>\n      <td>0.994967</td>\n      <td>0.000159</td>\n      <td>0.001130</td>\n      <td>0.000186</td>\n      <td>0.000240</td>\n      <td>...</td>\n      <td>0.028344</td>\n      <td>0.998878</td>\n      <td>0.001122</td>\n      <td>0.999532</td>\n      <td>0.000100</td>\n      <td>0.000090</td>\n      <td>0.000086</td>\n      <td>0.000067</td>\n      <td>0.000078</td>\n      <td>0.000047</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>Редко сюда забегаю. Маленький магазинчик, это ...</td>\n      <td>22</td>\n      <td>Редко сюда забегаю. Маленький магазинчик, это ...</td>\n      <td>Редко сюда забегаю. Маленький магазинчик, это ...</td>\n      <td>0.948751</td>\n      <td>0.000679</td>\n      <td>0.000612</td>\n      <td>0.019587</td>\n      <td>0.001260</td>\n      <td>...</td>\n      <td>0.232759</td>\n      <td>0.995381</td>\n      <td>0.004619</td>\n      <td>0.998663</td>\n      <td>0.000173</td>\n      <td>0.000143</td>\n      <td>0.000500</td>\n      <td>0.000154</td>\n      <td>0.000271</td>\n      <td>0.000096</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4</td>\n      <td>Сложно найти торговом центре. А магазин норм</td>\n      <td>7</td>\n      <td>Сложно найти торговом центре. А магазин норм</td>\n      <td>Сложно найти торговом центре. А магазин норм</td>\n      <td>0.565123</td>\n      <td>0.006904</td>\n      <td>0.000616</td>\n      <td>0.007276</td>\n      <td>0.004125</td>\n      <td>...</td>\n      <td>0.053535</td>\n      <td>0.998884</td>\n      <td>0.001117</td>\n      <td>0.999540</td>\n      <td>0.000078</td>\n      <td>0.000102</td>\n      <td>0.000090</td>\n      <td>0.000073</td>\n      <td>0.000074</td>\n      <td>0.000043</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3</td>\n      <td>После ремонта магазин нутри стал ещ лучше. Бол...</td>\n      <td>18</td>\n      <td>После ремонта магазин нутри стал ещ лучше. Бол...</td>\n      <td>После ремонта магазин нутри стал ещ лучше. Бол...</td>\n      <td>0.993944</td>\n      <td>0.000154</td>\n      <td>0.000255</td>\n      <td>0.000182</td>\n      <td>0.000107</td>\n      <td>...</td>\n      <td>0.120662</td>\n      <td>0.999014</td>\n      <td>0.000986</td>\n      <td>0.999063</td>\n      <td>0.000261</td>\n      <td>0.000107</td>\n      <td>0.000220</td>\n      <td>0.000092</td>\n      <td>0.000182</td>\n      <td>0.000075</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 410 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# TFIDF","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import matplotlib.cm as cm\n\n# from sklearn.cluster import MiniBatchKMeans\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.decomposition import PCA\n# from sklearn.manifold import TSNE\n\n# tfidf = TfidfVectorizer(\n#     min_df = 5,\n#     max_df = 0.85,\n#     max_features = 8000,\n# )\n# tfidf.fit(train_data.summary)\n# text = tfidf.transform(train_data.summary)\n\n# def find_optimal_clusters(data, max_k):\n#     iters = range(2, max_k+1, 2)\n    \n#     sse = []\n#     for k in iters:\n#         sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n#         print('Fit {} clusters'.format(k))\n        \n#     f, ax = plt.subplots(1, 1)\n#     ax.plot(iters, sse, marker='o')\n#     ax.set_xlabel('Cluster Centers')\n#     ax.set_xticks(iters)\n#     ax.set_xticklabels(iters)\n#     ax.set_ylabel('SSE')\n#     ax.set_title('SSE by Cluster Center Plot')\n\n# def plot_tsne_pca(data, labels):\n#     max_label = max(labels)\n#     max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)\n    \n#     pca = PCA(n_components=2).fit_transform(np.asarray(data[max_items,:].todense()))\n#     tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(np.asarray(data[max_items,:].todense())))\n    \n    \n#     idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)\n#     label_subset = labels[max_items]\n#     label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n    \n#     f, ax = plt.subplots(1, 2, figsize=(14, 6))\n    \n#     ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n#     ax[0].set_title('PCA Cluster Plot')\n    \n#     ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n#     ax[1].set_title('TSNE Cluster Plot')\n\n# find_optimal_clusters(text, 20)\n\n# plot_tsne_pca(text, clusters)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:12:52.636734Z","iopub.execute_input":"2023-11-05T10:12:52.637104Z","iopub.status.idle":"2023-11-05T10:12:54.779103Z","shell.execute_reply.started":"2023-11-05T10:12:52.637074Z","shell.execute_reply":"2023-11-05T10:12:54.777941Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=1000,\n                                 min_df=0.01,\n                                 use_idf=True, tokenizer=token_and_stem, ngram_range=(1,3))\nget_ipython().magic('time tfidf_matrix = tfidf_vectorizer.fit_transform(titles)')\nprint(tfidf_matrix.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_clusters = 5\n\n# Метод к-средних - KMeans\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=num_clusters)\nget_ipython().magic('time km.fit(tfidf_matrix)')\nidx = km.fit(tfidf_matrix)\nclusters = km.labels_.tolist()\n\nprint(clusters)\nprint (km.labels_)\n\n# MiniBatchKMeans\nfrom sklearn.cluster import MiniBatchKMeans\n\nmbk  = MiniBatchKMeans(init='random', n_clusters=num_clusters) #(init='k-means++', ‘random’ or an ndarray)\nmbk.fit_transform(tfidf_matrix)\n%time mbk.fit(tfidf_matrix)\nminiclusters = mbk.labels_.tolist()\nprint (mbk.labels_)\n\n\n# DBSCAN\nfrom sklearn.cluster import DBSCAN\nget_ipython().magic('time db = DBSCAN(eps=0.3, min_samples=10).fit(tfidf_matrix)')\nlabels = db.labels_\nlabels.shape\nprint(labels)\n\n# Аггломеративная класстеризация\nfrom sklearn.cluster import AgglomerativeClustering\n\nagglo1 = AgglomerativeClustering(n_clusters=num_clusters, affinity='euclidean') #affinity можно выбрать любое или попробовать все по очереди: cosine, l1, l2, manhattan\nget_ipython().magic('time answer = agglo1.fit_predict(tfidf_matrix.toarray())')\nanswer.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP profiler","metadata":{}},{"cell_type":"code","source":"!pip install -U nlp-profiler > installer_log.txt","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:32:15.973333Z","iopub.execute_input":"2023-11-05T10:32:15.973741Z","iopub.status.idle":"2023-11-05T10:32:38.335276Z","shell.execute_reply.started":"2023-11-05T10:32:15.973704Z","shell.execute_reply":"2023-11-05T10:32:38.333880Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscattertext 0.1.19 requires gensim>=4.0.0, but you have gensim 3.8.1 which is incompatible.\ntexthero 1.1.0 requires spacy<3.0.0, but you have spacy 3.6.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from nlp_profiler.core import apply_text_profiling\n\nprofiled_text_dataframe = apply_text_profiling(train_data, 'text')","metadata":{"execution":{"iopub.status.busy":"2023-11-05T10:34:34.237038Z","iopub.execute_input":"2023-11-05T10:34:34.237422Z","iopub.status.idle":"2023-11-05T13:20:53.365653Z","shell.execute_reply.started":"2023-11-05T10:34:34.237390Z","shell.execute_reply":"2023-11-05T13:20:53.364232Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"final params: {'high_level': True, 'granular': True, 'grammar_check': False, 'spelling_check': True, 'parallelisation_method': 'default'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c104fb44ae954c1fbc5d80c0c4822c0c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da94096a51374fcb91e757208d38775a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c9f47a32b5b4f7a93e6be763890032e"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f47b9c067d24556bcf0ebf55d876a64"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30cf4b38cd584345aeffe0d6c0ad7de4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0204aca4fe34289971d97d327b5d50c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8444ad1ae0e54aa9b62a2cf4733d1fe0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4052e968cd17469e8c99bab6cdfb6f16"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8d253e0c6c24dd280044a7c1b4de460"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d6ac184e3554669a963135ee0397a35"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ac70bb1d8fa42b9953d75f21090bd5f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b16b84083d4c3fa350b58fa83f4fa5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6403be2b460646d6af98d0c37dad5a33"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08215eab5e6f4759a74b042b5c6bc9ed"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f853c3ddbab247fc991214bee436a767"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b28bceb50eb4545b725c4ea14580583"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33cd090a0d6843589017653db0e01c28"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f58bc842b21433ab2613459a3653f97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807de8453d4d490690432f89da2cce78"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47805b81a53f41b08f7fc95eeaf45993"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddc1d2133be14eaf8f74f5cd2c9dbaa8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a35fa816d54afabc79c72fecba8312"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3429207d57140cca3e544ce942497bb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c52f749f17f541908f5866e09ab1fffd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b38277f269478794b7b42f086374ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a9fe831932c402b9a19e6165e7ef977"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|                                                                                                         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e58e5c261a94c1a93fce258e22c5e92"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/_store_backends.py:207: CacheWarning: Unable to cache to disk. Possibly a race condition in the creation of the directory. Exception: cannot pickle '_hashlib.HMAC' object.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"profiled_text_dataframe","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:46:04.228259Z","iopub.execute_input":"2023-11-05T13:46:04.229121Z","iopub.status.idle":"2023-11-05T13:46:04.365323Z","shell.execute_reply.started":"2023-11-05T13:46:04.229079Z","shell.execute_reply":"2023-11-05T13:46:04.364153Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"                                                    text  sentences_count  \\\n0      Очень понравилось. Были начале марта собакой. ...                8   \n1      В целом магазин устраивает. Ассортимент позвол...                5   \n2                Очень открылась ка, далеко ехать рядом!                1   \n3      Пятрочка громко объявила заботится пенсионерах...                3   \n4      Тесно, вечная сутолока, рядами трудно разойтис...                3   \n...                                                  ...              ...   \n48635       Удобный, маленький ещ обновили другие пятрки                1   \n48636  Постоянно обман цене,написанна сумма акции ито...                2   \n48637  Очень хочется пожелать этому магазину стать та...                1   \n48638  Нравится ваш магазин, персонал одекватный, пор...                1   \n48639                                  Не очень персонал                1   \n\n       characters_count  spaces_count  count_words  duplicates_count  \\\n0                   203            28           27                 1   \n1                   327            38           45                 4   \n2                    39             5            6                 0   \n3                   204            25           27                 3   \n4                    85            11           12                 2   \n...                 ...           ...          ...               ...   \n48635                44             5            6                 0   \n48636               226            25           30                 3   \n48637                67             8            9                 0   \n48638                51             5            6                 1   \n48639                17             2            3                 0   \n\n       chars_excl_spaces_count  emoji_count  whole_numbers_count  \\\n0                          175            0                    0   \n1                          289            0                    0   \n2                           34            0                    0   \n3                          179            0                    0   \n4                           74            0                    0   \n...                        ...          ...                  ...   \n48635                       39            0                    0   \n48636                      201            0                    0   \n48637                       59            0                    0   \n48638                       46            0                    0   \n48639                       15            0                    0   \n\n       alpha_numeric_count  ...  noun_phase_count  sentiment_polarity_score  \\\n0                        0  ...                22                       0.0   \n1                        0  ...                40                       0.0   \n2                        0  ...                 6                       0.0   \n3                        0  ...                25                       0.0   \n4                        0  ...                11                       0.0   \n...                    ...  ...               ...                       ...   \n48635                    0  ...                 6                       0.0   \n48636                    0  ...                30                       0.0   \n48637                    0  ...                 9                       0.0   \n48638                    0  ...                 6                       0.0   \n48639                    0  ...                 3                       0.0   \n\n       sentiment_polarity  sentiment_polarity_summarised  \\\n0                 Neutral                        Neutral   \n1                 Neutral                        Neutral   \n2                 Neutral                        Neutral   \n3                 Neutral                        Neutral   \n4                 Neutral                        Neutral   \n...                   ...                            ...   \n48635             Neutral                        Neutral   \n48636             Neutral                        Neutral   \n48637             Neutral                        Neutral   \n48638             Neutral                        Neutral   \n48639             Neutral                        Neutral   \n\n       sentiment_subjectivity_score  sentiment_subjectivity  \\\n0                               0.0          Very objective   \n1                               0.0          Very objective   \n2                               0.0          Very objective   \n3                               0.0          Very objective   \n4                               0.0          Very objective   \n...                             ...                     ...   \n48635                           0.0          Very objective   \n48636                           0.0          Very objective   \n48637                           0.0          Very objective   \n48638                           0.0          Very objective   \n48639                           0.0          Very objective   \n\n      sentiment_subjectivity_summarised spelling_quality_score  \\\n0                             Objective               0.277778   \n1                             Objective               0.207547   \n2                             Objective               0.250000   \n3                             Objective               0.235294   \n4                             Objective               0.333333   \n...                                 ...                    ...   \n48635                         Objective               0.142857   \n48636                         Objective               0.230769   \n48637                         Objective               0.100000   \n48638                         Objective               0.333333   \n48639                         Objective               0.000000   \n\n       spelling_quality spelling_quality_summarised  \n0            Pretty bad                         Bad  \n1            Pretty bad                         Bad  \n2            Pretty bad                         Bad  \n3            Pretty bad                         Bad  \n4            Pretty bad                         Bad  \n...                 ...                         ...  \n48635        Pretty bad                         Bad  \n48636        Pretty bad                         Bad  \n48637         Quite bad                         Bad  \n48638        Pretty bad                         Bad  \n48639          Very bad                         Bad  \n\n[48640 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentences_count</th>\n      <th>characters_count</th>\n      <th>spaces_count</th>\n      <th>count_words</th>\n      <th>duplicates_count</th>\n      <th>chars_excl_spaces_count</th>\n      <th>emoji_count</th>\n      <th>whole_numbers_count</th>\n      <th>alpha_numeric_count</th>\n      <th>...</th>\n      <th>noun_phase_count</th>\n      <th>sentiment_polarity_score</th>\n      <th>sentiment_polarity</th>\n      <th>sentiment_polarity_summarised</th>\n      <th>sentiment_subjectivity_score</th>\n      <th>sentiment_subjectivity</th>\n      <th>sentiment_subjectivity_summarised</th>\n      <th>spelling_quality_score</th>\n      <th>spelling_quality</th>\n      <th>spelling_quality_summarised</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Очень понравилось. Были начале марта собакой. ...</td>\n      <td>8</td>\n      <td>203</td>\n      <td>28</td>\n      <td>27</td>\n      <td>1</td>\n      <td>175</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>22</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.277778</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>В целом магазин устраивает. Ассортимент позвол...</td>\n      <td>5</td>\n      <td>327</td>\n      <td>38</td>\n      <td>45</td>\n      <td>4</td>\n      <td>289</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>40</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.207547</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Очень открылась ка, далеко ехать рядом!</td>\n      <td>1</td>\n      <td>39</td>\n      <td>5</td>\n      <td>6</td>\n      <td>0</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.250000</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Пятрочка громко объявила заботится пенсионерах...</td>\n      <td>3</td>\n      <td>204</td>\n      <td>25</td>\n      <td>27</td>\n      <td>3</td>\n      <td>179</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>25</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.235294</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Тесно, вечная сутолока, рядами трудно разойтис...</td>\n      <td>3</td>\n      <td>85</td>\n      <td>11</td>\n      <td>12</td>\n      <td>2</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>11</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.333333</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48635</th>\n      <td>Удобный, маленький ещ обновили другие пятрки</td>\n      <td>1</td>\n      <td>44</td>\n      <td>5</td>\n      <td>6</td>\n      <td>0</td>\n      <td>39</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.142857</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>48636</th>\n      <td>Постоянно обман цене,написанна сумма акции ито...</td>\n      <td>2</td>\n      <td>226</td>\n      <td>25</td>\n      <td>30</td>\n      <td>3</td>\n      <td>201</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>30</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.230769</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>48637</th>\n      <td>Очень хочется пожелать этому магазину стать та...</td>\n      <td>1</td>\n      <td>67</td>\n      <td>8</td>\n      <td>9</td>\n      <td>0</td>\n      <td>59</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>9</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.100000</td>\n      <td>Quite bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>48638</th>\n      <td>Нравится ваш магазин, персонал одекватный, пор...</td>\n      <td>1</td>\n      <td>51</td>\n      <td>5</td>\n      <td>6</td>\n      <td>1</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.333333</td>\n      <td>Pretty bad</td>\n      <td>Bad</td>\n    </tr>\n    <tr>\n      <th>48639</th>\n      <td>Не очень персонал</td>\n      <td>1</td>\n      <td>17</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>Neutral</td>\n      <td>Neutral</td>\n      <td>0.0</td>\n      <td>Very objective</td>\n      <td>Objective</td>\n      <td>0.000000</td>\n      <td>Very bad</td>\n      <td>Bad</td>\n    </tr>\n  </tbody>\n</table>\n<p>48640 rows × 24 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"profiled_text_dataframe.to_csv(\"feature_profiler_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:49:17.247762Z","iopub.execute_input":"2023-11-05T13:49:17.248198Z","iopub.status.idle":"2023-11-05T13:49:18.218201Z","shell.execute_reply.started":"2023-11-05T13:49:17.248165Z","shell.execute_reply":"2023-11-05T13:49:18.216879Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"markdown","source":"# CLasses","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-10-31T13:56:50.609192Z","iopub.execute_input":"2023-10-31T13:56:50.609591Z","iopub.status.idle":"2023-10-31T13:56:50.657674Z","shell.execute_reply.started":"2023-10-31T13:56:50.609557Z","shell.execute_reply":"2023-10-31T13:56:50.656658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nMAX_LEN = 50\nBATCH_SIZE = 64","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T13:56:51.500027Z","iopub.execute_input":"2023-10-31T13:56:51.500959Z","iopub.status.idle":"2023-10-31T13:56:51.507224Z","shell.execute_reply.started":"2023-10-31T13:56:51.500914Z","shell.execute_reply":"2023-10-31T13:56:51.506253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:44:22.630179Z","iopub.execute_input":"2023-10-31T14:44:22.630673Z","iopub.status.idle":"2023-10-31T14:44:22.927327Z","shell.execute_reply.started":"2023-10-31T14:44:22.630639Z","shell.execute_reply":"2023-10-31T14:44:22.926361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label encoding","metadata":{}},{"cell_type":"markdown","source":"# Cleaning","metadata":{}},{"cell_type":"code","source":"%aimport nltk.corpus.reader.bracket_parse\n%autoreload 0\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:44:37.973336Z","iopub.execute_input":"2023-10-31T14:44:37.974526Z","iopub.status.idle":"2023-10-31T14:44:37.98789Z","shell.execute_reply.started":"2023-10-31T14:44:37.974477Z","shell.execute_reply":"2023-10-31T14:44:37.987056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:44:40.19084Z","iopub.execute_input":"2023-10-31T14:44:40.191636Z","iopub.status.idle":"2023-10-31T14:45:25.52016Z","shell.execute_reply.started":"2023-10-31T14:44:40.191596Z","shell.execute_reply":"2023-10-31T14:45:25.519347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\ndef show_count_by_rate(data, rate_name = None, name = \"Data\"):\n    fig = px.histogram(data, x=\"num_words\", color=rate_name, title=f\"Number of words in {name} by rate\")\n    fig.update_layout(bargap=0.2)\n\n    fig.show()\n    \nshow_count_by_rate(train_data, rate_name = \"rate\", name = 'Train_data')\nshow_count_by_rate(test_data, name = 'Test_data')","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:45:25.521988Z","iopub.execute_input":"2023-10-31T14:45:25.522662Z","iopub.status.idle":"2023-10-31T14:45:25.671015Z","shell.execute_reply.started":"2023-10-31T14:45:25.522625Z","shell.execute_reply":"2023-10-31T14:45:25.670082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# удалим все 0 и те, чье количество меньше 30\ntrain_data = train_data[train_data['num_words'] != 0]\n# train_data = train_data[train_data['num_words'] < 30]","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:45:41.270307Z","iopub.execute_input":"2023-10-31T14:45:41.270738Z","iopub.status.idle":"2023-10-31T14:45:41.284651Z","shell.execute_reply.started":"2023-10-31T14:45:41.270707Z","shell.execute_reply":"2023-10-31T14:45:41.283548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test split","metadata":{}},{"cell_type":"code","source":"# train_split, val_split = train_test_split(train_data[train_data['rate'] != 4], test_size=0.15, random_state=42, \n#                                           shuffle = True, stratify=train_data[train_data['rate'] != 4]['rate'])\n\ntrain_split, val_split = train_test_split(train_data, test_size=0.15, random_state=42, \n                                          shuffle = True, stratify=train_data['rate'])","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:45:46.213648Z","iopub.execute_input":"2023-10-31T14:45:46.214031Z","iopub.status.idle":"2023-10-31T14:45:46.246988Z","shell.execute_reply.started":"2023-10-31T14:45:46.213998Z","shell.execute_reply":"2023-10-31T14:45:46.246134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.hist(train_split['rate'], bins=10, alpha=0.5, label='Train Split')\nplt.hist(val_split['rate'], bins=10, alpha=0.5, label='Validation Split')\n\nplt.xlabel('Rate')\nplt.ylabel('Frequency')\nplt.legend()\nplt.title('Histogram of Rates for Train and Validation Splits')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:45:47.779642Z","iopub.execute_input":"2023-10-31T14:45:47.780521Z","iopub.status.idle":"2023-10-31T14:45:48.144503Z","shell.execute_reply.started":"2023-10-31T14:45:47.780487Z","shell.execute_reply":"2023-10-31T14:45:48.143492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nweights = compute_class_weight(class_weight='balanced', classes=np.unique(train_split['rate']), y=train_split['rate'])\nweight_tensor = torch.FloatTensor(weights)\nprint(weights)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:45:49.288978Z","iopub.execute_input":"2023-10-31T14:45:49.289379Z","iopub.status.idle":"2023-10-31T14:45:49.30776Z","shell.execute_reply.started":"2023-10-31T14:45:49.289345Z","shell.execute_reply":"2023-10-31T14:45:49.306727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading tokenizer from pretrained","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    \"cointegrated/rubert-tiny2\", truncation=True, do_lower_case=True)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:45:52.086533Z","iopub.execute_input":"2023-10-31T14:45:52.086987Z","iopub.status.idle":"2023-10-31T14:45:52.289182Z","shell.execute_reply.started":"2023-10-31T14:45:52.086954Z","shell.execute_reply":"2023-10-31T14:45:52.288083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating datasets and dataloaders","metadata":{}},{"cell_type":"code","source":"train_dataset = FiveDataset(train_split, tokenizer, MAX_LEN)\nval_dataset = FiveDataset(val_split, tokenizer, MAX_LEN)\ntest_dataset = FiveDataset(test_data, tokenizer, MAX_LEN)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:46:05.410495Z","iopub.execute_input":"2023-10-31T14:46:05.411487Z","iopub.status.idle":"2023-10-31T14:46:05.422502Z","shell.execute_reply.started":"2023-10-31T14:46:05.411444Z","shell.execute_reply":"2023-10-31T14:46:05.421168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 100\nBATCH_SIZE = 400","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:46:05.964538Z","iopub.execute_input":"2023-10-31T14:46:05.965524Z","iopub.status.idle":"2023-10-31T14:46:05.971833Z","shell.execute_reply.started":"2023-10-31T14:46:05.965485Z","shell.execute_reply":"2023-10-31T14:46:05.970638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_params = {\"batch_size\": BATCH_SIZE,\n                \"shuffle\": True,\n                \"num_workers\": 0\n                }\n\ntest_params = {\"batch_size\": BATCH_SIZE,\n               \"shuffle\": False,\n               \"num_workers\": 0\n               }\n\ntrain_dataloader = DataLoader(train_dataset, **train_params)\nval_dataloader = DataLoader(val_dataset, **test_params)\ntest_dataloader = DataLoader(test_dataset, **test_params)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:46:06.517407Z","iopub.execute_input":"2023-10-31T14:46:06.51825Z","iopub.status.idle":"2023-10-31T14:46:06.527596Z","shell.execute_reply.started":"2023-10-31T14:46:06.518217Z","shell.execute_reply":"2023-10-31T14:46:06.526553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading pretrained model from Huggingface","metadata":{}},{"cell_type":"code","source":"config = {\n    \"num_classes\": len(np.unique(train_split['rate'])),\n    \"dropout_rate\": 0.1\n}\nmodel = ModelForClassification(\n    \"cointegrated/rubert-tiny2\",\n    config=config\n)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:46:07.887923Z","iopub.execute_input":"2023-10-31T14:46:07.88891Z","iopub.status.idle":"2023-10-31T14:46:08.289826Z","shell.execute_reply.started":"2023-10-31T14:46:07.888874Z","shell.execute_reply":"2023-10-31T14:46:08.288981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Trainer object and fitting the model","metadata":{}},{"cell_type":"code","source":"trainer_config = {\n    \"lr\": 3e-4,\n    \"n_epochs\": 3,\n    \"weight_decay\": 1e-6,\n    \"batch_size\": BATCH_SIZE,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"seed\": 42,\n}\nt = Trainer(trainer_config, class_weights=weight_tensor)\n# ,class_weights=weight_tensor","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:46:09.90588Z","iopub.execute_input":"2023-10-31T14:46:09.906737Z","iopub.status.idle":"2023-10-31T14:46:09.920318Z","shell.execute_reply.started":"2023-10-31T14:46:09.906695Z","shell.execute_reply":"2023-10-31T14:46:09.918959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.fit(\n    model,\n    train_dataloader,\n    val_dataloader\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:46:10.763116Z","iopub.execute_input":"2023-10-31T14:46:10.764035Z","iopub.status.idle":"2023-10-31T14:48:19.04622Z","shell.execute_reply.started":"2023-10-31T14:46:10.764Z","shell.execute_reply":"2023-10-31T14:48:19.045305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"t.save(\"best_baseline_model.ckpt\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:26:44.960478Z","iopub.execute_input":"2023-10-31T14:26:44.960921Z","iopub.status.idle":"2023-10-31T14:26:45.19635Z","shell.execute_reply.started":"2023-10-31T14:26:44.960888Z","shell.execute_reply":"2023-10-31T14:26:45.195357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load pretrained Model","metadata":{}},{"cell_type":"code","source":"t = Trainer.load(\"best_baseline_model.ckpt\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:26:48.26766Z","iopub.execute_input":"2023-10-31T14:26:48.268447Z","iopub.status.idle":"2023-10-31T14:26:48.877352Z","shell.execute_reply.started":"2023-10-31T14:26:48.268402Z","shell.execute_reply":"2023-10-31T14:26:48.876474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get testset predictions\n","metadata":{}},{"cell_type":"code","source":"predictions = t.predict(test_dataloader)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:49:43.825153Z","iopub.execute_input":"2023-10-31T14:49:43.825736Z","iopub.status.idle":"2023-10-31T14:49:50.387273Z","shell.execute_reply.started":"2023-10-31T14:49:43.825696Z","shell.execute_reply":"2023-10-31T14:49:50.386167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_classes = [np.argmax(probabilities) + 1 for probabilities in predictions]","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:49:50.389275Z","iopub.execute_input":"2023-10-31T14:49:50.389691Z","iopub.status.idle":"2023-10-31T14:49:50.4471Z","shell.execute_reply.started":"2023-10-31T14:49:50.389655Z","shell.execute_reply":"2023-10-31T14:49:50.446357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission\n","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(os.path.join(PATH, \"sample_submission.csv\"))\nsample_submission[\"rate\"] = predicted_classes\n# sample_submission.rate = le.inverse_transform(sample_submission.rate)\nsample_submission.head()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:49:50.448179Z","iopub.execute_input":"2023-10-31T14:49:50.448485Z","iopub.status.idle":"2023-10-31T14:49:50.520997Z","shell.execute_reply.started":"2023-10-31T14:49:50.448459Z","shell.execute_reply":"2023-10-31T14:49:50.520099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-31T14:49:50.5227Z","iopub.execute_input":"2023-10-31T14:49:50.522981Z","iopub.status.idle":"2023-10-31T14:49:50.55259Z","shell.execute_reply.started":"2023-10-31T14:49:50.522956Z","shell.execute_reply":"2023-10-31T14:49:50.551593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train conf matrix","metadata":{}},{"cell_type":"code","source":"predictions_val = t.predict(val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:49:55.945166Z","iopub.execute_input":"2023-10-31T14:49:55.945602Z","iopub.status.idle":"2023-10-31T14:49:59.807246Z","shell.execute_reply.started":"2023-10-31T14:49:55.945567Z","shell.execute_reply":"2023-10-31T14:49:59.806202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_classes_val = [np.argmax(probabilities) + 1 for probabilities in predictions_val]","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:49:59.809523Z","iopub.execute_input":"2023-10-31T14:49:59.810341Z","iopub.status.idle":"2023-10-31T14:49:59.85013Z","shell.execute_reply.started":"2023-10-31T14:49:59.810301Z","shell.execute_reply":"2023-10-31T14:49:59.849428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef evaluate_classification_metrics(y_true, y_pred, model_name):\n    cm = confusion_matrix(y_true, y_pred)\n    print(f\"Classification Report for {model_name}:\\n\", classification_report(y_true, y_pred))\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n\nevaluate_classification_metrics(predicted_classes_val, le.inverse_transform(val_split['rate']), \"val dataset\")","metadata":{"execution":{"iopub.status.busy":"2023-10-31T14:49:59.851133Z","iopub.execute_input":"2023-10-31T14:49:59.851398Z","iopub.status.idle":"2023-10-31T14:50:00.182108Z","shell.execute_reply.started":"2023-10-31T14:49:59.851375Z","shell.execute_reply":"2023-10-31T14:50:00.181163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}