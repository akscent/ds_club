{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7056730,"sourceType":"datasetVersion","datasetId":4051877}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-28T02:34:33.672789Z","iopub.execute_input":"2023-11-28T02:34:33.673631Z","iopub.status.idle":"2023-11-28T02:34:35.250840Z","shell.execute_reply.started":"2023-11-28T02:34:33.673582Z","shell.execute_reply":"2023-11-28T02:34:35.249442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_parquet('/kaggle/input/sibalfahack/Siberian Alfa Hack Materials/Siberian Alfa Hack Materials/train.parquet')\ntest_df = pd.read_parquet('/kaggle/input/sibalfahack/Siberian Alfa Hack Materials/Siberian Alfa Hack Materials/test.parquet')\ndf = pd.concat([train_df, test_df], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T02:38:33.032882Z","iopub.execute_input":"2023-11-28T02:38:33.033271Z","iopub.status.idle":"2023-11-28T02:38:37.246590Z","shell.execute_reply.started":"2023-11-28T02:38:33.033242Z","shell.execute_reply":"2023-11-28T02:38:37.245295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-28T02:38:40.287068Z","iopub.execute_input":"2023-11-28T02:38:40.287578Z","iopub.status.idle":"2023-11-28T02:38:40.298924Z","shell.execute_reply.started":"2023-11-28T02:38:40.287536Z","shell.execute_reply":"2023-11-28T02:38:40.297471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"class DataProcessor:\n    def __init__(self, data, test_data, target, list_drop_cols):\n        self.data = data\n        self.test_data = test_data\n        self.cat_cols = list(data.select_dtypes(include=['object']).columns) + list(data.select_dtypes(include=['category']).columns)\n        self.target = target\n        \n    def rm_spare_cols(self, list_drop_cols=0):\n        \"\"\"\n        Удаление лишних столбцов.\n            Params:\n                list_drop_cols: содержит список наименований столбцов\n        \"\"\"\n        if list_drop_cols: self.data.drop(self.list_drop_cols, axis=1, inplace=True)\n        \n    def rm_NaN_treshold(self, treshold = 200000):\n        \"\"\"\n        Удаление всех столбцов, NAN в которых больше заданного порога.\n        \"\"\"\n        self.data = self.data.columns[self.data.isnull().sum() > treshold]\n        \n    def rm_cols_small_target(self, threshold=100):\n        columns_to_check = self.data.columns.difference([self.target])\n        selected_columns = []\n        for column in columns_to_check:\n            df_subset = pd.DataFrame({column: self.data[column], target_column: self.data[self.target]})\n            df_subset = df_subset.dropna()\n            count_ones = df_subset[self.target].sum()\n            if count_ones < threshold:\n                selected_columns.append(column)\n\n        return self.data.drop(columns = selected_columns, inplace = true)\n        \n    def rm_or_fill_values(self, rm = True):\n        \"\"\"\n        Удаление строк с отрицательными значениями или замена ячеек на NaN.\n        \"\"\"\n        if self.cat_cols is not None:\n            if rm:\n                self.data[~self.cat_cols] = self.data[~self.cat_cols][self.data[~self.cat_cols].le(0).all(axis=1)]\n            else:\n                self.data[~self.cat_cols].mask(self.data < 0, 0, inplace = True)\n    \n    def clean_data(self, drop_NaN = False):\n        \"\"\"\n        Метод для очистки данных. Удаление дубликатов, обработка пропущенных значений\n        \"\"\"\n        self.data = self.data.drop_duplicates()\n        if drop_NaN: self.data = self.data.dropna()\n            \n    def fill_missing_cats(self):\n        \"\"\"\n        Method for filling misssing values of categorical features with NaN rows on -1\n        \"\"\"\n        categorical_features = self.data.select_dtypes(include=['category']).columns\n\n        for feature in categorical_features:\n            self.data[feature] = self.data[feature].astype('object')\n            self.data[feature].fillna(-1, inplace=True)\n            self.data[feature] = self.data[feature].astype('category')\n\n        return self.data\n        \n    def fill_nan_with_group_math(self, cat_cols, math = \"mean\"):\n        \"\"\"\n        Method for filling NaN of numerical features by group math variable.\n        If group has 0 values, only NaN, than filling with 0\n            Params:\n                cat_cols: list categorical columns for missing values\n                math: mean/median or other\n        \"\"\"\n        filled_df = self.data.copy()\n        for column in self.data.select_dtypes(include='number').columns:\n            if self.data[column].isnull().any():\n                temp_df = pd.DataFrame({column: self.data[column]})\n                for cat_col in cat_cols:\n                    temp_df[cat_col] = df[cat_col]\n\n                group_math = temp_df.groupby(cat_cols)[column].transform(math)\n                filled_df[column] = filled_df[column].combine_first(group_math).fillna(0)\n\n        return filled_df\n\n    def preprocess_data(self, list_drop_cols = 0, rmnan_treshold = 200000, treshold = 0, rm_or_fill_values = 0\n                       drop_NaN = False, fill_missing_cats = False, fill_cats_NaN = 0, fill_cats_math = 0):\n        \"\"\"\n        Method for preprocessing of pd.DataFrame\n        Params:\n            list_drop_cols: 0 - False, list_drop_cols - list with rm cols\n            rmnan_treshold: treshold with counts of NaN in column for drop\n            treshold: 0 - False, treshold with count of target for drop columns\n            rm_or_fill_values: 0 - False, 1 - fill values with NaN, 2 - remove rows with negatives\n            drop_NaN: False - do not drop rows with NAN, True - drop rows with NAN\n            fill_missing_cats: False - do not missing, True - missing\n            fill_cats_NaN: 0 - False, list with cat_cols for filling by group math\n            fill_cats_math: 0 - False, string with name math variable. For example 'mean'\n            \n        \"\"\"\n        if list_drop_cols: self.rm_spare_cols(list_drop_cols)\n        self.data[self.cat_cols] = self.data[self.cat_cols].astype(\"category\")\n        self.rm_NaN_treshold(rmnan_treshold)\n        if treshold: self.rm_cols_small_target(threshold)\n        if rm_or_fill_values == 1:\n            self.rm_or_fill_values(rm = True)\n        elif rm_or_fill_values == 2:\n            self.rm_or_fill_values(rm = False)\n        self.clean_data(drop_NaN)\n        if fill_missing_cats: self.data = self.fill_missing_cats(fill_missing_cats)\n        if fill_cats_NaN:\n            if fill_cats_math: self.fill_nan_with_group_math(cat_cols = self.cat_cols, math = fill_cats_math)\n            else: raise ValueError(\"Invalid math variable for 'math'. Use math variable from pandas math\")\n        \n        \n    def visualize_cols_distribution(self, cols_list):\n        \"\"\"\n        Визуализация списка столбцов по категориям\n        \"\"\"\n        import matplotlib as plt\n        labels = self.target\n        for column in cols_list:\n            df_subset = pd.DataFrame({column: self.data[column], target_column: self.data[labels]})\n            df_subset = df_subset.dropna()\n            count_ones = df_subset[target_column].sum()\n            plt.figure(figsize=(10, 6))\n            sns.histplot(data=df_subset, x=column, hue=target_column, bins=30, kde=True)\n            plt.title(f'Distribution of \"{column}\" (Total Label: {count_ones} ones)')\n            plt.xlabel(column)\n            plt.ylabel('Count')\n            plt.legend(title=target_column)\n            plt.show()\n        \n    def le_encode_categorical_features(self):\n        \"\"\"\n        Кодирует категориальные признаки числовыми значениями\n        \"\"\"\n        for col in self.cat_cols:\n            le = LabelEncoder()\n            self.data[col] = le.fit_transform(self.data[col])\n\n    def visualize_feature(self, col):\n        \"\"\"\n        Метод для визуализации признака\n        \"\"\"\n        if col in self.cat_cols or col == 'total_target':\n            fig = px.pie(self.data, names=col, title=f'Распределение признака {col}')\n        else:\n            fig = px.histogram(self.data, x=col, title=f'Распределение признака {col}', labels={col: col})\n        fig.show()\n    \n    def visualize_target(self, target):\n        \"\"\"\n        Метод для визуализации распределения таргетов\n        \"\"\"\n        fig = px.histogram(self.data, x=target, color=target,\n                           title='Распределение таргета', labels={target: target},\n                           category_orders={target: [0, 1]}, barmode='overlay')\n        fig.update_layout(showlegend=False)\n        fig.show()\n    \n    def visualize_correlations(self, max_corr, min_corr):\n        \"\"\"\n        Метод для визуализации скоррелированных фичей\n        \"\"\"\n        labels_to_drop = {(self.data.columns[i], self.data.columns[j]) for i in range(self.data.shape[1]) for j in range(i + 1)}\n        au_corr = self.data.corr().unstack()\n        au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n        filtercorr = au_corr[((au_corr >= min_corr) & (au_corr <= max_corr)) | ((au_corr <= -min_corr) & (au_corr >= -max_corr)) & (au_corr !=1.0)]\n        au_corr = filtercorr.unstack(level=0)\n        fig = px.imshow(au_corr, aspect=\"auto\")\n        fig.update_layout(font=dict(size=8))\n        fig.show()\n        \n    def get_churn_category(self, group_by_column, target_column):\n        \"\"\"\n        Метод для расчета процента оттока по категориям для категориальных фичей\n        \"\"\"\n        grouped_data = self.data.groupby(group_by_column, as_index=False).agg({target_column: ['sum', 'count']})\n        grouped_data.columns = [group_by_column, 'Churn_Sum', 'Churn_Count']\n        grouped_data['Churn_Percentage'] = 100 * grouped_data['Churn_Sum'] / grouped_data['Churn_Count']\n        grouped_data = grouped_data.sort_values('Churn_Percentage').reset_index(drop=True)\n        return grouped_data","metadata":{"execution":{"iopub.status.busy":"2023-11-28T02:39:55.108068Z","iopub.execute_input":"2023-11-28T02:39:55.108490Z","iopub.status.idle":"2023-11-28T02:39:55.142276Z","shell.execute_reply.started":"2023-11-28T02:39:55.108460Z","shell.execute_reply":"2023-11-28T02:39:55.141235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass AdversarialValidation:\n    from scipy import stats\n    from catboost import CatBoostClassifier\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import roc_auc_score\n    \n    def __init__(self, train_df, test_df, features, target):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.features = features\n        self.target = target\n        self.cat_cols = self.train_df.select_dtypes(include=['object']).columns\n        \n    def create_adversarial_dataset(self):\n        # Добавляем метку \"0\" для train_df и \"1\" для test_df\n        self.train_df[\"is_train\"] = 0\n        self.test_df[\"is_train\"] = 1\n\n        # Объединяем оба набора данных\n        adv_data = pd.concat([self.train_df, self.test_df], axis=0)\n        \n        # Заполняем пропущенные значения в категориальных признаках\n        for col in self.cat_cols:\n            mode_value = adv_data[col].mode()[0]\n            adv_data[col].fillna(mode_value, inplace=True)\n\n        return adv_data\n\n    def run_adversarial_validation(self, params=None):\n        # Создаем adversarial dataset\n        adv_data = self.create_adversarial_dataset()\n\n        # Разделяем на обучающий и тестовый наборы для adversarial validation\n        X_adv = adv_data[self.features]\n        y_adv = adv_data[\"is_train\"]\n\n        X_train_adv, X_valid_adv, y_train_adv, y_valid_adv = train_test_split(\n            X_adv, y_adv, test_size=0.2, random_state=42\n        )\n\n        # Инициализируем и обучаем модель CatBoost\n        model_params = params or {\n            \"objective\": \"Logloss\",\n            \"iterations\": 300,\n            \"learning_rate\": 0.05,\n            \"depth\": 6,\n            \"l2_leaf_reg\": 3,\n            \"verbose\": 100,\n        }\n\n        model = CatBoostClassifier(**model_params)\n        model.fit(X_train_adv, y_train_adv, cat_features=list(self.cat_cols), eval_set=(X_valid_adv, y_valid_adv), early_stopping_rounds=50, verbose_eval=100)\n\n        # Предсказываем вероятности для adversarial dataset\n        adv_pred = model.predict_proba(X_valid_adv)[:, 1]\n\n        # Вычисляем ROC AUC для adversarial validation\n        auc_score = roc_auc_score(y_valid_adv, adv_pred)\n        print(f\"Adversarial Validation ROC AUC Score: {auc_score}\")\n        return auc_score\n\n    def stats_for_cols(self, )\n        features_list = self.test_df.select_dtypes(include=['number']).columns.values.tolist()\n        bad_features, good_features = [], []\n        for feature in features_list:\n            statistic, p_value = stats.kstest(self.train_df[feature].dropna(), self.test_df[feature].dropna())\n            if statistic > 0.1 and p_value < 0.05:\n                print(\"KS test value: %.3f\" %statistic, \"with a p-value %.2f\" %p_value, \"for the feature\", feature)\n                bad_features.append(feature)\n            else:\n                good_features.append(feature)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_processor = DataProcessor(train_df, 'total_target')","metadata":{"execution":{"iopub.status.busy":"2023-11-25T12:40:57.341174Z","iopub.execute_input":"2023-11-25T12:40:57.341622Z","iopub.status.idle":"2023-11-25T12:40:57.390568Z","shell.execute_reply.started":"2023-11-25T12:40:57.341584Z","shell.execute_reply":"2023-11-25T12:40:57.388995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_processor.visualize_target('total_target')","metadata":{"execution":{"iopub.status.busy":"2023-11-25T12:40:58.131271Z","iopub.execute_input":"2023-11-25T12:40:58.132066Z","iopub.status.idle":"2023-11-25T12:40:58.250161Z","shell.execute_reply.started":"2023-11-25T12:40:58.132012Z","shell.execute_reply":"2023-11-25T12:40:58.249006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_processor.visualize_feature('segment')","metadata":{"execution":{"iopub.status.busy":"2023-11-25T12:41:02.99396Z","iopub.execute_input":"2023-11-25T12:41:02.994801Z","iopub.status.idle":"2023-11-25T12:41:04.150498Z","shell.execute_reply.started":"2023-11-25T12:41:02.994751Z","shell.execute_reply":"2023-11-25T12:41:04.148671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_processor.visualize_feature('sum_a_oper_1m')","metadata":{"execution":{"iopub.status.busy":"2023-11-25T13:17:38.727736Z","iopub.execute_input":"2023-11-25T13:17:38.728172Z","iopub.status.idle":"2023-11-25T13:17:38.917572Z","shell.execute_reply.started":"2023-11-25T13:17:38.728138Z","shell.execute_reply":"2023-11-25T13:17:38.916645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_processor.visualize_correlations(min_corr=0.7, max_corr=0.9999)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T12:41:34.601335Z","iopub.execute_input":"2023-11-25T12:41:34.601824Z","iopub.status.idle":"2023-11-25T12:41:45.914973Z","shell.execute_reply.started":"2023-11-25T12:41:34.601787Z","shell.execute_reply":"2023-11-25T12:41:45.913901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_processor.get_churn_category(group_by_column='segment', target_column='target_2')","metadata":{"execution":{"iopub.status.busy":"2023-11-25T13:45:59.296208Z","iopub.execute_input":"2023-11-25T13:45:59.296668Z","iopub.status.idle":"2023-11-25T13:45:59.385391Z","shell.execute_reply.started":"2023-11-25T13:45:59.296628Z","shell.execute_reply":"2023-11-25T13:45:59.384423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}