{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/akscent/ods-boosting?scriptVersionId=150697171\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install shap optuna -q > installer_log.txt\nimport cudf\n# %load_ext cudf.pandas \nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\nimport os\nimport shap\nimport optuna\nimport copy\nfrom optuna.samplers import TPESampler\nfrom optuna.integration import XGBoostPruningCallback\nfrom catboost import CatBoostClassifier, Pool\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import KFold, train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import f1_score, log_loss\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import VotingClassifier\nfrom optuna.integration import CatBoostPruningCallback\n\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\noptuna.logging.set_verbosity(optuna.logging.CRITICAL)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-13T12:54:51.0254Z","iopub.execute_input":"2023-11-13T12:54:51.026272Z","iopub.status.idle":"2023-11-13T12:55:13.879231Z","shell.execute_reply.started":"2023-11-13T12:54:51.026236Z","shell.execute_reply":"2023-11-13T12:55:13.878256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Train Data","metadata":{}},{"cell_type":"code","source":"pr_train = pd.read_csv('/kaggle/input/ods-huawei/feature_profiler_train.csv')\ntrain = pd.read_csv('/kaggle/input/ods-huawei/new_train.csv')\nf_train = pd.read_csv('/kaggle/input/ods-huawei/feature_train.csv')\np_train = pd.read_csv('/kaggle/input/ods-huawei/train_proba_tf.csv')\nlstm_train = pd.read_csv('/kaggle/input/ods-huawei/lstm_features_train.csv')\ntrain = pd.concat([f_train['rate'], train, p_train, train, pr_train, lstm_train], axis=1)\ntrain = train.loc[:,~train.columns.duplicated()]\nnon_numeric_columns = train.select_dtypes(include=['float64', 'int64']).columns\n\ny = train['rate']\nX = train[non_numeric_columns].drop(columns = 'rate')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-13T13:23:16.674362Z","iopub.execute_input":"2023-11-13T13:23:16.674862Z","iopub.status.idle":"2023-11-13T13:23:30.025251Z","shell.execute_reply.started":"2023-11-13T13:23:16.674829Z","shell.execute_reply":"2023-11-13T13:23:30.024419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature as needed\n\nlist_columns = ['MOOD_1', 'MOOD_0', 'MOOD_2', 'Num_NOUN', 'Num_ADJ', 'EMOTION_2',\n       'TOXIC_0', 'spelling_quality_score', 'EMOTION_1', 'TOXIC_1',\n       'EMOTION_4', 'EMOTION_5', 'LABEL_10', 'Num_VERB', 'sentences_count',\n       'LABEL_15', 'chars_excl_spaces_count', 'LABEL_2', 'LABEL_5',\n       'EMOTION_0', 'LABEL_8', 'non_alpha_numeric_count', 'LABEL_9',\n       'EMOTION_6', 'LABEL_11', 'characters_count', 'LABEL_17', 'LABEL_1',\n       'LABEL_13', 'LABEL_14', 'LABEL_18', 'LABEL_95', 'LABEL_0',\n       'noun_phase_count', 'LABEL_6', 'LABEL_103', 'LABEL_127', 'LABEL_210',\n       'LABEL_119', 'LABEL_177', 'LABEL_118', 'LABEL_89', 'LABEL_20',\n       'LABEL_100', 'LABEL_97', 'LABEL_334', 'LABEL_247', 'LABEL_215',\n       'LABEL_169', 'num_unique_words', '1 star', '2 stars', '3 stars',\n       '4 stars', '5 stars', 'spaces_count', 'count_words', 'duplicates_count',\n       'emoji_count', 'whole_numbers_count', 'alpha_numeric_count',\n       'punctuations_count', 'stop_words_count', 'dates_count',\n       'sentiment_polarity_score', 'sentiment_subjectivity_score',\n       'Star_LSTM_1', 'Star_LSTM_2', 'Star_LSTM_3', 'Star_LSTM_4',\n       'Star_LSTM_5']","metadata":{"execution":{"iopub.status.busy":"2023-11-13T13:00:24.025677Z","iopub.execute_input":"2023-11-13T13:00:24.026036Z","iopub.status.idle":"2023-11-13T13:00:24.033309Z","shell.execute_reply.started":"2023-11-13T13:00:24.026008Z","shell.execute_reply":"2023-11-13T13:00:24.032318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Catboost search best hyperparameters with Optuna","metadata":{}},{"cell_type":"code","source":"\ndef fit_catboost(trial, train, val):\n    X_train, y_train = train\n    X_val, y_val = val\n    train_dataset = Pool(data=X_train, label=y_train)\n    eval_dataset = Pool(data=X_val, label=y_val)\n\n    param = {\n        'iterations' : 3000,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.01),\n        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 2, 50),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.8),\n        \"auto_class_weights\": trial.suggest_categorical(\"auto_class_weights\", [\"SqrtBalanced\", \"Balanced\", \"None\"]),\n        \"depth\": trial.suggest_int(\"depth\", 3, 9),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"eval_metric\": \"Accuracy\",\n    }\n\n    pruning_callback = CatBoostPruningCallback(trial, \"Accuracy\")\n    clf = CatBoostClassifier(**param, task_type=\"CPU\", early_stopping_rounds=1000, random_seed=42)\n    clf.fit(train_dataset, eval_set=eval_dataset, verbose=500, callbacks=[pruning_callback], plot=False)\n    pruning_callback.check_pruned()\n    y_pred = clf.predict(X_val)\n    return clf, y_pred\n\n\ndef objective(trial, return_models=False):\n    n_splits = 10\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    scores, models = [], []\n    \n    for train_idx, valid_idx in kf.split(X):\n        train_data = X.iloc[train_idx, :], y.iloc[train_idx]\n        valid_data = X.iloc[valid_idx, :], y.iloc[valid_idx]\n        model, y_pred = fit_catboost(trial, train_data, valid_data)\n        scores.append(f1_score(y_pred, valid_data[1], average='weighted'))\n        models.append(model)\n        break\n        \n    result = np.mean(scores)\n    \n    if return_models:\n        return result, models\n    else:\n        return result\n    \nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective,\n               n_trials=300,\n               n_jobs = -1,\n               show_progress_bar=True)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\n\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n    \ncatboost_params = {\n    \"learning_rate\": 0.0031241744378306684,\n    \"l2_leaf_reg\": 21,\n    \"colsample_bylevel\": 0.4661705497287475,\n    \"auto_class_weights\": None,\n    \"depth\": 6,\n    \"boosting_type\": \"Ordered\",\n    \"eval_metric\": \"Accuracy\",\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-13T06:07:08.287821Z","iopub.execute_input":"2023-11-13T06:07:08.288283Z","iopub.status.idle":"2023-11-13T06:07:08.301779Z","shell.execute_reply.started":"2023-11-13T06:07:08.288246Z","shell.execute_reply":"2023-11-13T06:07:08.300717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_catboost(X, y, params):\n    n_splits = 2\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    scores, models = [], []\n\n    for train_idx, valid_idx in kf.split(X):\n        X_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n        X_val, y_val = X.iloc[valid_idx, :], y.iloc[valid_idx]\n\n        train_dataset = Pool(data=X_train, label=y_train)\n        eval_dataset = Pool(data=X_val, label=y_val)\n\n        clf = CatBoostClassifier(**params, task_type=\"CPU\", iterations=5000,\n                                early_stopping_rounds=2000, random_seed=42)\n        clf.fit(train_dataset, eval_set=eval_dataset, verbose=500, plot=False)\n\n        y_pred = clf.predict(X_val)\n        scores.append(f1_score(y_pred, y_val, average='weighted'))\n        models.append(clf)\n\n    mean_score = np.mean(scores)\n    best_model = models[np.argmax(scores)]\n\n    return mean_score, best_model\n\n\nparams = {\n    \"learning_rate\": 0.0031241744378306684,\n    \"l2_leaf_reg\": 21,\n    \"colsample_bylevel\": 0.4661705497287475,\n    \"auto_class_weights\": None,\n    \"depth\": 6,\n    \"boosting_type\": \"Ordered\",\n    \"eval_metric\": \"Accuracy\",\n}\n\nmean_score, best_model = train_catboost(X, y, params)\nprint(\"Mean F1 Score:\", mean_score)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:55:47.882246Z","iopub.execute_input":"2023-11-13T07:55:47.883051Z","iopub.status.idle":"2023-11-13T08:30:26.074285Z","shell.execute_reply.started":"2023-11-13T07:55:47.883018Z","shell.execute_reply":"2023-11-13T08:30:26.072915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\n\nexplainer = shap.TreeExplainer(best_model)\n\ntrain_dataset = Pool(data=X, label=y)\nshap_values = explainer.shap_values(train_dataset)\n\nshap.summary_plot(shap_values, X, max_display=25)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T08:40:14.526642Z","iopub.execute_input":"2023-11-13T08:40:14.527096Z","iopub.status.idle":"2023-11-13T08:41:04.239729Z","shell.execute_reply.started":"2023-11-13T08:40:14.527051Z","shell.execute_reply":"2023-11-13T08:41:04.238629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost search best hyperparameters with Optuna","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    global clf\n\n    param_grid = {\n        'max_depth': trial.suggest_int('max_depth', 6, 15),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n        'n_estimators': trial.suggest_int('n_estimators', 2000, 12000),\n        'lambda': trial.suggest_float('lambda', 1e-3, 1e3),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n        'min_child_weight': trial.suggest_int('min_child_weight', 300, 2000),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.10, 0.5),\n    }\n\n    clf = XGBClassifier(objective='multi:softprob', use_label_encoder=False,\n                        num_class=5, gpu_id=0, tree_method='gpu_hist', **param_grid)\n\n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    f1s = []\n\n    for train_idx, valid_idx in kf.split(X, y):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y[train_idx], y[valid_idx]\n\n        X_train = np.log(X_train + 1)\n        X_valid = np.log(X_valid + 1)\n        X_train = RobustScaler().fit_transform(X_train)\n        X_valid = RobustScaler().fit_transform(X_valid)\n\n        pruning_callback = XGBoostPruningCallback(trial, 'validation_1-mlogloss')\n        clf.fit(X_train, y_train,\n                eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                eval_metric='mlogloss', verbose=3000, callbacks=[pruning_callback])\n\n        y_pred = clf.predict(X_valid)\n        f1 = f1_score(y_valid, y_pred, average='weighted')\n        f1s.append(f1)\n\n    return np.mean(f1s)\n\ndef callback(study, trial):\n    global best_classifier\n    if study.best_trial == trial:\n        best_classifier = clf\n\ntrain_time = 3 * 60 * 60  # 3 h * 60 m * 60 s\n\nstudy = optuna.create_study(direction='maximize', sampler=TPESampler())\nstudy.optimize(objective, timeout=train_time, callbacks=[callback])\n\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('  Value: {}'.format(trial.value))\nprint('  Params: ')\nfor key, value in trial.params.items():\n    print('    {}: {}'.format(key, value))\n\nxgb_params = {\n    'max_depth': 13,\n    'subsample': 0.692900501887427,\n    'n_estimators': 10000,\n    'lambda': 27.658047515722444,\n    'learning_rate': 0.010222318009404707,\n    'min_child_weight': 1003,\n    'colsample_bytree': 0.17027213976623792,\n    'early_stopping_rounds': 3000,\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T04:35:54.422088Z","iopub.execute_input":"2023-11-12T04:35:54.422373Z","iopub.status.idle":"2023-11-12T07:45:35.772707Z","shell.execute_reply.started":"2023-11-12T04:35:54.422348Z","shell.execute_reply":"2023-11-12T07:45:35.771695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# важность фич по весу\nfeature_importance_weight = best_xgb_model.get_booster().get_score(importance_type='weight')\ntop_features_weight = [feature for feature, _ in sorted(feature_importance_weight.items(), key=lambda x: x[1], reverse=True)[:25]]\n\n# важность фич по gain\nfeature_importance_gain = best_xgb_model.get_booster().get_score(importance_type='gain')\ntop_features_gain = [feature for feature, _ in sorted(feature_importance_gain.items(), key=lambda x: x[1], reverse=True)[:25]]\n\n# важность фич по покрытию\nfeature_importance_cover = best_xgb_model.get_booster().get_score(importance_type='cover')\ntop_features_cover = [feature for feature, _ in sorted(feature_importance_cover.items(), key=lambda x: x[1], reverse=True)[:25]]\nunique_top_features = list(set(top_features_weight + top_features_gain + top_features_cover))\nprint(unique_top_features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM search best hyperparameters with Optuna","metadata":{}},{"cell_type":"code","source":"\ndef objective(trial, X, y):\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'multiclass',\n        'num_class': 5,\n        'n_estimators': trial.suggest_int('n_estimators', 2000, 5000),\n        'device_type': 'gpu',\n        'metric': 'multi_logloss',\n        'min_child_weight': trial.suggest_int('min_child_weight', 300, 2000),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.10, 0.5),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n    }\n\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    scores = []\n\n    for train_idx, valid_idx in tqdm(kf.split(X), total=n_splits, desc='Training'):\n        X_train, X_valid = X.iloc[train_idx, :], X.iloc[valid_idx, :]\n        y_train, y_valid = y[train_idx], y[valid_idx]\n        \n        X_train.columns = X_train.columns.str.replace(' ', '_')\n        X_valid.columns = X_valid.columns.str.replace(' ', '_')\n\n        clf = lgb.LGBMClassifier(**params)\n        clf.fit(X_train, y_train, verbose=0)\n        y_pred = clf.predict(X_valid)\n        f1 = f1_score(y_valid, y_pred, average='weighted')\n        scores.append(f1)\n\n    mean_score = sum(scores) / len(scores)\n\n    return mean_score\n\nstudy = optuna.create_study(direction='maximize', sampler=TPESampler())\nstudy.optimize(lambda trial: objective(trial, X, y), n_trials=100)\nbest_lgbm_params = study.best_params\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('  Value: {}'.format(trial.value))\nprint('  Params: ')\nfor key, value in trial.params.items():\n    print('    {}: {}'.format(key, value))\n\nbest_lgbm_params = {\n    'n_estimators': 3000,\n    'min_child_weight': 351,\n    'colsample_bytree': 0.40874204382658685,\n    'num_leaves': 23,\n    'learning_rate': 0.010433728530130512,\n}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train best models","metadata":{}},{"cell_type":"code","source":"def train_model_with_cv(X, y, model_type, params, n_splits=5):\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    f1s = []\n    best_f1 = 0\n    best_model = None\n\n    for train_idx, valid_idx in kf.split(X, y):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y[train_idx], y[valid_idx]\n\n        if model_type == 'XGBClassifier':\n            model = XGBClassifier(objective='multi:softprob', use_label_encoder=False,\n                                  num_class=len(np.unique(y)), gpu_id=0, tree_method='gpu_hist', **params)\n            model.fit(X_train, y_train,\n                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                  eval_metric='mlogloss', verbose=1000)\n            \n        elif model_type == 'LGBM':\n            model = LGBMClassifier(objective='multi:softprob',\n                                  num_class=len(np.unique(y)), **params);\n            model.fit(X_train, y_train, verbose=1000)\n            \n        elif model_type == 'CatboostClassifer':\n            train_dataset = Pool(data=X_train, label=y_train)\n            eval_dataset = Pool(data=X_valid, label=y_valid)\n            model = CatBoostClassifier(**params);\n            model.fit(train_dataset, eval_set=eval_dataset, verbose=1000, plot=False)\n\n        y_pred = model.predict(X_valid)\n        f1 = f1_score(y_valid, y_pred, average='weighted')\n        f1s.append(f1)\n\n        if f1 > best_f1:\n            best_f1 = f1\n            best_model = copy.deepcopy(model)\n\n    average_f1 = np.mean(f1s)\n    return average_f1, best_model\n\nmodel_xgb = 'XGBClassifier'\nmodel_catb = 'CatboostClassifer'\nmodel_lgbm = 'LGBM'\n\nbest_xgb_model = train_model_with_cv(X, y, model_xgb, xgb_params)\nbest_lgbm_model = train_model_with_cv(X, y, model_lgbm, lgbm_params)\nbest_catb_model = train_model_with_cv(X, y, model_catb, catb_params)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T04:22:09.960361Z","iopub.execute_input":"2023-11-13T04:22:09.96138Z","iopub.status.idle":"2023-11-13T04:22:09.974163Z","shell.execute_reply.started":"2023-11-13T04:22:09.961342Z","shell.execute_reply":"2023-11-13T04:22:09.97289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_lgbm_model.booster_.save_model('lgbm_model.txt')\nbest_xgb_model.save_model('xgboos_model.cbm')\nbest_catb_model.save_model('catboost_model.cbm')","metadata":{"execution":{"iopub.status.busy":"2023-11-13T04:30:14.969587Z","iopub.execute_input":"2023-11-13T04:30:14.97001Z","iopub.status.idle":"2023-11-13T04:30:15.24848Z","shell.execute_reply.started":"2023-11-13T04:30:14.969977Z","shell.execute_reply":"2023-11-13T04:30:15.247303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Test Data","metadata":{}},{"cell_type":"code","source":"PATH = \"/kaggle/input/ods-huawei/\"\ntest_data = pd.read_csv(os.path.join(PATH, \"test_cleaned.csv\"))\nadj_test_data = pd.read_csv(os.path.join(PATH, \"adj_test_data.csv\"))\nadd_feature = pd.read_csv(os.path.join(PATH, \"test_proba_tf.csv\"))\nlabels_test_data = pd.read_csv(os.path.join(PATH, \"labels_test_data.csv\"))\nprofiler_add_feature = pd.read_csv(os.path.join(PATH, \"profiled_text_data_test.csv\"))\nmood_toxic_test = pd.read_csv(os.path.join(PATH, \"mood_toxic_test.csv\"))\nlstm_proba_test = pd.read_csv(os.path.join(PATH, \"lstm_features_test.csv\"))\n\ntest = pd.concat([test_data, add_feature, adj_test_data, profiler_add_feature, mood_toxic_test, lstm_proba_test, labels_test_data], axis=1)\n\ntest = test.loc[:,~test.columns.duplicated()]\nnon_numeric_columns = test.select_dtypes(include=['float64', 'int64']).columns\n\nX = test[non_numeric_columns]\nX = X[list_columns]","metadata":{"execution":{"iopub.status.busy":"2023-11-13T13:01:45.551574Z","iopub.execute_input":"2023-11-13T13:01:45.551962Z","iopub.status.idle":"2023-11-13T13:01:47.622244Z","shell.execute_reply.started":"2023-11-13T13:01:45.551934Z","shell.execute_reply":"2023-11-13T13:01:47.621453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test submission","metadata":{}},{"cell_type":"code","source":"lgbm = lgb.Booster(model_file=os.path.join(PATH, 'lgbm_model.txt'))\ncatboost = CatBoostClassifier()\ncatboost.load_model(os.path.join(PATH, 'catboost_model.cbm'))\nxgboost = XGBClassifier()\nxgboost.load_model(os.path.join(PATH, 'xgboos_model.cbm'))\n\ndef ensemble_predict(lgbm, catboost, xgboost, X):\n    lgbm_pred = lgbm.predict(X, num_iteration=lgbm.best_iteration)\n    catboost_pred = catboost.predict_proba(X)\n    xgboost_pred = xgboost.predict_proba(X)\n\n    ensemble_proba = np.mean([lgbm_pred, catboost_pred, xgboost_pred], axis=0)\n    ensemble_pred = np.argmax(ensemble_proba, axis=1)\n\n    return ensemble_pred\n\nensemble_pred = ensemble_predict(lgbm, catboost, xgboost, X)\nensemble_pred\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T13:17:48.208504Z","iopub.execute_input":"2023-11-13T13:17:48.208945Z","iopub.status.idle":"2023-11-13T13:17:56.426208Z","shell.execute_reply.started":"2023-11-13T13:17:48.208908Z","shell.execute_reply":"2023-11-13T13:17:56.425361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble_pred = ensemble_pred + 1","metadata":{"execution":{"iopub.status.busy":"2023-11-13T13:18:19.872196Z","iopub.execute_input":"2023-11-13T13:18:19.872553Z","iopub.status.idle":"2023-11-13T13:18:19.880245Z","shell.execute_reply.started":"2023-11-13T13:18:19.872525Z","shell.execute_reply":"2023-11-13T13:18:19.879376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(os.path.join(PATH, \"sample_submission.csv\"))\nsample_submission[\"rate\"] = ensemble_pred\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T13:19:29.344134Z","iopub.execute_input":"2023-11-13T13:19:29.344496Z","iopub.status.idle":"2023-11-13T13:19:29.384192Z","shell.execute_reply.started":"2023-11-13T13:19:29.344464Z","shell.execute_reply":"2023-11-13T13:19:29.38327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train confusion matrix","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ndef evaluate_classification_metrics(y_true, y_pred, model_name):\n    cm = confusion_matrix(y_true, y_pred)\n    print(f\"Classification Report for {model_name}:\\n\", classification_report(y_true, y_pred))\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n\nensemble_pred = ensemble_predict(lgbm, catboost, xgboost, X)\nevaluate_classification_metrics(ensemble_pred, y, \"train dataset\")","metadata":{"execution":{"iopub.status.busy":"2023-11-13T13:25:52.946089Z","iopub.execute_input":"2023-11-13T13:25:52.946925Z","iopub.status.idle":"2023-11-13T13:25:53.317919Z","shell.execute_reply.started":"2023-11-13T13:25:52.946885Z","shell.execute_reply":"2023-11-13T13:25:53.316976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}