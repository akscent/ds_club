{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/akscent/feature-extraction-train?scriptVersionId=150693529\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pymorphy2 cleantext -U pip setuptools wheel nlp_profiler textblob pymystem3 > installer_log.txt\n!pip install spacy > installer_log.txt\nimport os\nimport sys\nimport torch\nimport json\nimport spacy\nimport io\nimport ru_core_news_md\nimport shap\nshap.initjs()\nimport pandas as pd\nimport numpy as np\n\nfrom numpy import asarray\nfrom collections import Counter\nfrom typing import Dict\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, AutoModel, MBartTokenizer, MBartForConditionalGeneration, BertTokenizer, BertForSequenceClassification\nfrom textblob import TextBlob\nfrom nlp_profiler.core import apply_text_profiling\nfrom pymystem3 import Mystem\nfrom nltk.corpus import stopwords\nfrom catboost import CatBoostClassifier\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data load","metadata":{}},{"cell_type":"code","source":"PATH = \"/kaggle/input/ods-huawei/\"\ntrain_data = pd.read_csv(os.path.join(PATH, \"train.csv\"))\ntest_data = pd.read_csv(os.path.join(PATH, \"test.csv\"))\nle = LabelEncoder()\ntrain_data.rate = le.fit_transform(train_data.rate)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:19:43.033117Z","iopub.execute_input":"2023-11-05T06:19:43.03346Z","iopub.status.idle":"2023-11-05T06:19:43.594763Z","shell.execute_reply.started":"2023-11-05T06:19:43.033433Z","shell.execute_reply":"2023-11-05T06:19:43.593753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean text","metadata":{}},{"cell_type":"code","source":"ru_stopwords = stopwords.words('russian')\ndigits = [str(i) for i in range(10)]\n\nTOKEN_RE = re.compile(r'[а-яё!.,?%]+')\nlemmatizer = pymorphy2.MorphAnalyzer()\n\ndef is_valid_word(word):\n    if not word[0].isdigit() and word not in ru_stopwords:\n        parsed_word = lemmatizer.normal_forms(word)[0]\n        return parsed_word\n    return False\n\ndef text_cleaning(text):\n    text = re.sub(r'[^a-zA-Zа-яА-Я0-9\\s.,!?]', '', text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    words = text.split()\n    cleaned_words = [word for word in words[:512] if is_valid_word(word) and len(word) < 15]\n    cleaned_text = ' '.join(cleaned_words)\n    return cleaned_text\n\ntqdm.pandas()\ntrain_data['text'] = train_data['text'].progress_apply(text_cleaning)\ntest_data['text'] = test_data['text'].progress_apply(text_cleaning)\n\ntrain_data[\"num_words\"] = train_data[\"text\"].apply(\n    lambda x: len(str(x).split()))\ntest_data[\"num_words\"] = test_data[\"text\"].apply(\n    lambda x: len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:19:43.596959Z","iopub.execute_input":"2023-11-05T06:19:43.597285Z","iopub.status.idle":"2023-11-05T06:20:01.348683Z","shell.execute_reply.started":"2023-11-05T06:19:43.597246Z","shell.execute_reply":"2023-11-05T06:20:01.342341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del zero string\ntrain_data = train_data[train_data['num_words'] != 0]\ntest_data = test_data[test_data['num_words'] != 0]","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.351384Z","iopub.status.idle":"2023-11-05T06:20:01.351919Z","shell.execute_reply.started":"2023-11-05T06:20:01.35166Z","shell.execute_reply":"2023-11-05T06:20:01.351687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean fucking words\n\ndef remove_infrequent_words(dataset, min_count=3):\n    word_counter = Counter()\n    for text in dataset:\n        words = text.split()\n        word_counter.update(words)\n    infrequent_words = [word for word, count in word_counter.items() if count < min_count]\n    def remove_infrequent(text):\n        words = text.split()\n        cleaned_words = [word for word in words if word not in infrequent_words]\n        cleaned_text = ' '.join(cleaned_words)\n        return cleaned_text\n    cleaned_dataset = [remove_infrequent(text) for text in tqdm(dataset, desc=\"Cleaning text\")]\n\n    return cleaned_dataset\n\ntrain_data = remove_infrequent_words(train_data['text'].tolist())\ntest_data = remove_infrequent_words(test_data['text'].tolist())\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.353418Z","iopub.status.idle":"2023-11-05T06:20:01.353951Z","shell.execute_reply.started":"2023-11-05T06:20:01.353696Z","shell.execute_reply":"2023-11-05T06:20:01.353721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace nan\n\ndef replace_nan_with_text(row):\n    if pd.isna(row['cleaned_text']):\n        return row['text']\n    return row['cleaned_text']\n\ntrain_data['cleaned_text'] = train_data.progress_apply(replace_nan_with_text, axis=1)\ntest_data['cleaned_text'] = test_data.progress_apply(replace_nan_with_text, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.359476Z","iopub.status.idle":"2023-11-05T06:20:01.359837Z","shell.execute_reply.started":"2023-11-05T06:20:01.359672Z","shell.execute_reply":"2023-11-05T06:20:01.359689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def truncate_text(text, max_words=512):\n    words = text.split()\n    if len(words) > max_words:\n        truncated_text = ' '.join(words[:max_words])\n    else:\n        truncated_text = text\n    return truncated_text\n\ntqdm.pandas()\ntrain_data['cleaned_text'] = train_data['cleaned_text'].progress_apply(truncate_text)\ntest_data['cleaned_text'] = test_data['cleaned_text'].progress_apply(truncate_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.361828Z","iopub.status.idle":"2023-11-05T06:20:01.362175Z","shell.execute_reply.started":"2023-11-05T06:20:01.362007Z","shell.execute_reply":"2023-11-05T06:20:01.362024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text summarizer","metadata":{}},{"cell_type":"code","source":"# идея суммирования текста в более короткий текст\n\nmodel_name = \"IlyaGusev/mbart_ru_sum_gazeta\"\ntokenizer = MBartTokenizer.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\n\ndef summary_rows(article_text):\n    input_ids = tokenizer(\n        [article_text],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    )[\"input_ids\"]\n\n    output_ids = model.generate(\n        input_ids=input_ids,\n        no_repeat_ngram_size=4\n    )[0]\n\n    summary = tokenizer.decode(output_ids, skip_special_tokens=True)\n    return summary\n\ndef text_summary(text):\n    if isinstance(text, str) and text.strip() and len(str(text).split()) > 150:\n        return summary_rows(text)\n    else:\n        return text\n    \n\ntrain_data['summary'] = train_data['cleaned_text'].progress_apply(text_summary)\ntest_data['summary'] = test_data['cleaned_text'].progress_apply(text_summary)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.363174Z","iopub.status.idle":"2023-11-05T06:20:01.363491Z","shell.execute_reply.started":"2023-11-05T06:20:01.363333Z","shell.execute_reply":"2023-11-05T06:20:01.363348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature generation","metadata":{}},{"cell_type":"markdown","source":"## What industry?","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"apanc/russian-sensitive-topics\")\ntokenizer = AutoTokenizer.from_pretrained(\"apanc/russian-sensitive-topics\")\ntokenizer.padding = True\ntokenizer.truncation = True\ntokenizer.max_length = 512\npipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=torch.device(\"cuda:0\"))\n\ndef make_pipe(text):\n    return pipe(text, return_all_scores=True)\n\ntqdm.pandas()\ntrain_data['theme_labels'] = train_data['summary'].progress_apply(make_pipe)\n\ndef extract_label_probs(row):\n    label_probs = [label['score'] for label in row[0]]\n    return label_probs\n\ntrain_data['label_probs'] = train_data['theme_labels'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].apply(pd.Series).add_prefix('LABEL_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['theme_labels']","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.366207Z","iopub.status.idle":"2023-11-05T06:20:01.366526Z","shell.execute_reply.started":"2023-11-05T06:20:01.366361Z","shell.execute_reply":"2023-11-05T06:20:01.366376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text tone","metadata":{}},{"cell_type":"code","source":"# тональность текста\npipe = pipeline(model=\"seara/rubert-tiny2-russian-sentiment\", device=torch.device(\"cuda:0\"))\n\ntqdm.pandas()\ntrain_data['mood'] = train_data['summary'].progress_apply(make_pipe)\n\ntrain_data['label_probs'] = train_data['mood'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('MOOD_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['mood']","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.373498Z","iopub.status.idle":"2023-11-05T06:20:01.373983Z","shell.execute_reply.started":"2023-11-05T06:20:01.373749Z","shell.execute_reply":"2023-11-05T06:20:01.373772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text toxicity","metadata":{}},{"cell_type":"code","source":"# токичность\n\npipe = pipeline(model=\"SkolkovoInstitute/russian_toxicity_classifier\", device=torch.device(\"cuda:0\"))\n\ntqdm.pandas()\ntrain_data['toxic'] = train_data['summary'].progress_apply(make_pipe)\n\ntrain_data['label_probs'] = train_data['toxic'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('TOXIC_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['toxic']","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.37938Z","iopub.status.idle":"2023-11-05T06:20:01.379758Z","shell.execute_reply.started":"2023-11-05T06:20:01.37958Z","shell.execute_reply":"2023-11-05T06:20:01.379599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Emotions","metadata":{}},{"cell_type":"code","source":"# эмоции\n\nLABELS = ['neutral', 'happiness', 'sadness', 'enthusiasm', 'fear', 'anger', 'disgust']\ntokenizer = AutoTokenizer.from_pretrained('Aniemore/rubert-tiny2-russian-emotion-detection')\nmodel = BertForSequenceClassification.from_pretrained('Aniemore/rubert-tiny2-russian-emotion-detection')\n\n@torch.no_grad()\ndef predict_emotion(text: str) -> str:\n    \"\"\"\n        We take the input text, tokenize it, pass it through the model, and then return the predicted label\n        :param text: The text to be classified\n        :type text: str\n        :return: The predicted emotion\n    \"\"\"\n    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n    outputs = model(**inputs)\n    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n    predicted = torch.argmax(predicted, dim=1).numpy()\n        \n    return LABELS[predicted[0]]\n\n@torch.no_grad()    \ndef predict_emotions(text: str) -> list:\n    \"\"\"\n        It takes a string of text, tokenizes it, feeds it to the model, and returns a dictionary of emotions and their\n        probabilities\n        :param text: The text you want to classify\n        :type text: str\n        :return: A dictionary of emotions and their probabilities.\n    \"\"\"\n    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n    outputs = model(**inputs)\n    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n    emotions_list = {}\n    for i in range(len(predicted.numpy()[0].tolist())):\n        emotions_list[LABELS[i]] = predicted.numpy()[0].tolist()[i]\n    return emotions_list\n\ntrain_data['toxic'] = train_data['summary'].progress_apply(predict_emotions)\n\ndef extract_label_probs(row):\n    label_probs = [row.get(label, 0.0) for label in LABELS]\n    return label_probs\n\ntrain_data['label_probs'] = train_data['toxic'].apply(extract_label_probs)\n\ntrain_data = pd.concat([train_data, train_data['label_probs'].progress_apply(pd.Series).add_prefix('EMOTION_')], axis=1)\n\ndel train_data['label_probs']\ndel train_data['toxic']","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.382385Z","iopub.status.idle":"2023-11-05T06:20:01.382747Z","shell.execute_reply.started":"2023-11-05T06:20:01.382569Z","shell.execute_reply":"2023-11-05T06:20:01.382587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"num_words_sum\"] = train_data[\"summary\"].apply(\n    lambda x: len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T11:05:12.226647Z","iopub.execute_input":"2023-11-05T11:05:12.226929Z","iopub.status.idle":"2023-11-05T11:05:12.327256Z","shell.execute_reply.started":"2023-11-05T11:05:12.226904Z","shell.execute_reply":"2023-11-05T11:05:12.326498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Other nlp imports","metadata":{}},{"cell_type":"code","source":"# ! textblob - обработка текста, генерация фич https://textblob.readthedocs.io/en/dev/quickstart.html - ничего интересного\n# ! еще одна библиотека для классификации текстов https://small-text.readthedocs.io/en/latest/ не подходит? для малкеньких текстов\n# ! полярность слов https://polyglot.readthedocs.io/en/latest/ - тоже? что уже получил из предобученных моделей\n# ! обработка фич https://github.com/jbesomi/texthero - плохо поддерживается\n# фичегенерация https://github.com/neomatrix369/nlp_profiler#Notebooks\n# классификация на других предобученных моделях, перечисленных у Алерона https://github.com/a-milenkin/Competitive_Data_Science/blob/main/notebooks/9.2.1%20-%20Text_Embeddings.ipynb\n# использовать эти ноутбуки для классификации https://github.com/e0xextazy/vkcup2022-first-stage/blob/main/inference.ipynb","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.39724Z","iopub.status.idle":"2023-11-05T06:20:01.397729Z","shell.execute_reply.started":"2023-11-05T06:20:01.39746Z","shell.execute_reply":"2023-11-05T06:20:01.397482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP profiler","metadata":{}},{"cell_type":"code","source":"profiled_text_dataframe = apply_text_profiling(train_data, 'text')","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:20:01.420107Z","iopub.status.idle":"2023-11-05T06:20:01.420444Z","shell.execute_reply.started":"2023-11-05T06:20:01.420278Z","shell.execute_reply":"2023-11-05T06:20:01.420294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SpaCy","metadata":{}},{"cell_type":"code","source":"tqdm.pandas()\ntrain_data[\"num_unique_words\"] = train_data[\"text\"].progress_apply(lambda x: len(set(str(x).split())))\ntrain_data[\"num_stopwords\"] = train_data[\"text\"].progress_apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T11:15:08.42389Z","iopub.execute_input":"2023-11-05T11:15:08.424813Z","iopub.status.idle":"2023-11-05T11:15:10.158024Z","shell.execute_reply.started":"2023-11-05T11:15:08.424769Z","shell.execute_reply":"2023-11-05T11:15:10.15712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features of POS\\o\n\nclass TextPOSAnalysis:\n    def __init__(self):\n        self.nlp_ru = ru_core_news_md.load()\n        self.df_pos = self.load_pos_table()\n        self.m = Mystem()\n\n    def load_pos_table(self):\n        table = \"\"\"\n        A       ADJ\n        ADV     ADV\n        ADVPRO  ADV\n        ANUM    ADJ\n        APRO    DET\n        COM     ADJ\n        CONJ    SCONJ\n        INTJ    INTJ\n        NONLEX  X\n        NUM     NUM\n        PART    PART\n        PR      ADP\n        S       NOUN\n        SPRO    PRON\n        UNKN    X\n        V       VERB\n        \"\"\"\n        table_file = io.StringIO(table)\n        df = pd.read_csv(table_file, sep=\"\\s+\", header=None, names=[\"token\", \"universal_pos\"])\n        return df\n\n    def get_universal_tag(self, word):\n        processed = self.m.analyze(word)[0]\n        lemma = processed[\"analysis\"][0][\"lex\"].lower().strip()\n        pos = processed[\"analysis\"][0][\"gr\"].split(',')[0]\n        pos = pos.split('=')[0].strip()\n        tagged = lemma + '_' + pos\n        return tagged\n\n    def add_tag(self, word):\n        word = self.get_universal_tag(word)\n        tag = word.split('_')[1]\n        tag = self.df_pos[self.df_pos['token'] == tag]['universal_pos'].values[0] if tag in self.df_pos['token'].values else tag\n        word = word.split('_')[0] + '_' + tag\n        return word\n\n    def analyze_text(self, text):\n        doc = self.nlp_ru(text)\n        num_adj = len([tok for tok in doc if tok.pos_ == 'ADJ'])\n        num_adv = len([tok for tok in doc if tok.pos_ == 'ADV'])\n        num_noun = len([tok for tok in doc if tok.pos_ == 'NOUN'])\n        num_verb = len([tok for tok in doc if tok.pos_ == 'VERB'])\n        return num_adj, num_noun, num_verb, num_adv\n\n    def analyze_texts(self, texts):\n        results = []\n        for text in texts:\n            results.append(self.analyze_text(text))\n        return pd.DataFrame(results, columns=[\"Num_ADJ\", \"Num_ADV\", \"Num_NOUN\", \"Num_VERB\"])\n\n\ntext_POS = TextPOSAnalysis()\nPOS_results = text_POS.analyze_texts(train_data['text'])\ntrain_data = pd.concat([train_data, POS_results], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T11:22:35.206268Z","iopub.execute_input":"2023-11-05T11:22:35.206637Z","iopub.status.idle":"2023-11-05T11:33:38.444881Z","shell.execute_reply.started":"2023-11-05T11:22:35.206607Z","shell.execute_reply":"2023-11-05T11:33:38.443825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"markdown","source":"## Shap","metadata":{}},{"cell_type":"code","source":"train_split, val_split = train_test_split(train_data, test_size=0.15, random_state=42, \n                                          shuffle = True, stratify=train_data['rate'])\ntrain_X = train_split.iloc[:, 5:441] \ntrain_Y = train_split['rate']\n\nval_X = val_split.iloc[:, 5:441] \nval_Y = val_split['rate']\n\ndel_columns = ['text', 'sentiment_polarity',\n       'sentiment_polarity_summarised',\n       'sentiment_subjectivity', 'sentiment_subjectivity_summarised',\n       'spelling_quality',\n       'spelling_quality_summarised']\n\ntrain_X = train_X.drop(columns = del_columns)\nval_X = val_X.drop(columns = del_columns)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:19:18.744966Z","iopub.execute_input":"2023-11-06T05:19:18.745433Z","iopub.status.idle":"2023-11-06T05:19:19.220403Z","shell.execute_reply.started":"2023-11-06T05:19:18.745398Z","shell.execute_reply":"2023-11-06T05:19:19.219089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = CatBoostClassifier(random_seed=9,\n                        thread_count=-1,\n                        use_best_model=True,\n                        bootstrap_type='Bernoulli')\n\nclf.fit(train_X, train_Y,\n        eval_set=(val_X, val_Y),\n        verbose=100,\n        plot=True,\n        early_stopping_rounds=1000)\n\nprint(clf.get_best_score())","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:20:51.492901Z","iopub.execute_input":"2023-11-06T05:20:51.493342Z","iopub.status.idle":"2023-11-06T05:27:51.8815Z","shell.execute_reply.started":"2023-11-06T05:20:51.493305Z","shell.execute_reply":"2023-11-06T05:27:51.880034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi = clf.get_feature_importance(prettified=True)[:100]","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:28:12.256188Z","iopub.execute_input":"2023-11-06T05:28:12.256685Z","iopub.status.idle":"2023-11-06T05:28:12.272037Z","shell.execute_reply.started":"2023-11-06T05:28:12.256646Z","shell.execute_reply":"2023-11-06T05:28:12.270408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X = train_X[fi['Feature Id'].to_list()]\nval_X = val_X[fi['Feature Id'].to_list()]","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:28:19.925097Z","iopub.execute_input":"2023-11-06T05:28:19.925615Z","iopub.status.idle":"2023-11-06T05:28:19.952556Z","shell.execute_reply.started":"2023-11-06T05:28:19.925579Z","shell.execute_reply":"2023-11-06T05:28:19.950896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Рекурсивный feature_selection Catboost","metadata":{}},{"cell_type":"code","source":"summary = clf.select_features(train_X, train_Y, \n                      eval_set=(val_X, val_Y),\n                      features_for_select='0-99',\n                      num_features_to_select=50,\n                      steps=1,\n                      train_final_model=False,\n                      logging_level='Silent')","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:28:26.512725Z","iopub.execute_input":"2023-11-06T05:28:26.514226Z","iopub.status.idle":"2023-11-06T05:30:26.454735Z","shell.execute_reply.started":"2023-11-06T05:28:26.514147Z","shell.execute_reply":"2023-11-06T05:30:26.453766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save new_train","metadata":{}},{"cell_type":"code","source":"summary['selected_features_names'].extend(['text', 'summary'])","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:35:28.847678Z","iopub.execute_input":"2023-11-06T05:35:28.848186Z","iopub.status.idle":"2023-11-06T05:35:28.854908Z","shell.execute_reply.started":"2023-11-06T05:35:28.84815Z","shell.execute_reply":"2023-11-06T05:35:28.853585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train = train_data[summary['selected_features_names']]\nnew_train.to_csv(\"new_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:35:47.650568Z","iopub.execute_input":"2023-11-06T05:35:47.652816Z","iopub.status.idle":"2023-11-06T05:35:53.891881Z","shell.execute_reply.started":"2023-11-06T05:35:47.652735Z","shell.execute_reply":"2023-11-06T05:35:53.890133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}