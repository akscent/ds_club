{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":91.411499,"end_time":"2022-10-09T03:36:04.617837","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-10-09T03:34:33.206338","version":"2.3.4"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":26266,"databundleVersionId":2030504,"sourceType":"competition"},{"sourceId":7053386,"sourceType":"datasetVersion","datasetId":4051877,"isSourceIdPinned":true},{"sourceId":7137138,"sourceType":"datasetVersion","datasetId":4118554}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/akscent/fitmlforcharn-prediction?scriptVersionId=154080459\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom catboost import CatBoostClassifier\nfrom matplotlib import pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom pprint import pprint\nfrom warnings import filterwarnings\n\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nfilterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\nfilterwarnings(\"ignore\", category=FutureWarning)\nfilterwarnings(\"ignore\", category=Warning)\nfilterwarnings(\"ignore\", category=DeprecationWarning)\n\ntry:\n    import cupy as cp\n    gpu_available = True\nexcept ImportError:\n    gpu_available = False\n\nn_jobs = -1 if gpu_available else 1","metadata":{"id":"6d5debb1","papermill":{"duration":1.93347,"end_time":"2022-10-09T03:34:44.544551","exception":false,"start_time":"2022-10-09T03:34:42.611081","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-07T13:46:04.06407Z","iopub.execute_input":"2023-12-07T13:46:04.064491Z","iopub.status.idle":"2023-12-07T13:46:04.073248Z","shell.execute_reply.started":"2023-12-07T13:46:04.064459Z","shell.execute_reply":"2023-12-07T13:46:04.072255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-11-26T15:49:18.187597Z","iopub.execute_input":"2023-11-26T15:49:18.188389Z","iopub.status.idle":"2023-11-26T15:49:18.193154Z","shell.execute_reply.started":"2023-11-26T15:49:18.188357Z","shell.execute_reply":"2023-11-26T15:49:18.192192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import wandb\n# #https://wandb.ai/authorize\n# wandb.login()","metadata":{"execution":{"iopub.status.busy":"2023-11-26T15:49:18.974533Z","iopub.execute_input":"2023-11-26T15:49:18.975282Z","iopub.status.idle":"2023-11-26T15:49:18.979243Z","shell.execute_reply.started":"2023-11-26T15:49:18.975239Z","shell.execute_reply":"2023-11-26T15:49:18.978223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CONFIG = dict ()\n\n# CONFIG['model_name'] = 'DJStacking'\n# print('Training configuration: ', CONFIG)\n\n# # Initialize W&B run\n# run = wandb.init(project='AlfaSibHack',\n#                  config=CONFIG,\n#                  group='DJStacking', \n#                  job_type='train')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T15:49:19.581089Z","iopub.execute_input":"2023-11-26T15:49:19.581454Z","iopub.status.idle":"2023-11-26T15:49:19.585587Z","shell.execute_reply.started":"2023-11-26T15:49:19.581427Z","shell.execute_reply":"2023-11-26T15:49:19.584623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{"id":"891fe076","papermill":{"duration":0.015378,"end_time":"2022-10-09T03:34:44.577856","exception":false,"start_time":"2022-10-09T03:34:44.562478","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/yandexcontest/train (2).csv')\ntest_df = pd.read_csv('/kaggle/input/yandexcontest/train (2).csv')\ndata = train_df\ny_col = [\"target\"]\n\n# x_train = pd.read_csv('/kaggle/input/sibalfahack/X_train_v1.csv')\n# y_train = pd.read_csv('/kaggle/input/sibalfahack/y_train_v1.csv')\n# test_df = pd.read_csv('/kaggle/input/sibalfahack/test_v1 (2).csv')\n# y_cols = [\"target\"]\n# data = x_train\n# data[y_cols] = y_train","metadata":{"execution":{"iopub.status.busy":"2023-12-07T13:46:15.641452Z","iopub.execute_input":"2023-12-07T13:46:15.64223Z","iopub.status.idle":"2023-12-07T13:46:15.661236Z","shell.execute_reply.started":"2023-12-07T13:46:15.642182Z","shell.execute_reply":"2023-12-07T13:46:15.660462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_cols = [\n#        'sum_b_oper_3m', 'cnt_b_oper_3m', 'sum_c_oper_3m', 'cnt_c_oper_3m',\n#        'sum_deb_d_oper_3m', 'cnt_deb_d_oper_3m', 'sum_cred_d_oper_3m',\n#        'cnt_cred_d_oper_3m', 'sum_deb_e_oper_3m', 'cnt_deb_e_oper_3m',\n#        'cnt_days_deb_e_oper_3m', 'sum_cred_e_oper_3m', 'cnt_cred_e_oper_3m',\n#        'cnt_days_cred_e_oper_3m', 'sum_deb_f_oper_3m', 'cnt_deb_f_oper_3m',\n#        'cnt_days_deb_f_oper_3m', 'sum_cred_f_oper_3m', 'cnt_cred_f_oper_3m',\n#        'cnt_days_cred_f_oper_3m', 'sum_deb_g_oper_3m', 'cnt_deb_g_oper_3m',\n#        'cnt_days_deb_g_oper_3m', 'sum_cred_g_oper_3m', 'cnt_cred_g_oper_3m',\n#        'cnt_days_cred_g_oper_3m', 'sum_deb_h_oper_3m', 'cnt_deb_h_oper_3m',\n#        'cnt_days_deb_h_oper_3m', 'sum_cred_h_oper_3m', 'cnt_cred_h_oper_3m',\n#        'cnt_days_cred_h_oper_3m'\n# ]\n\n# cat_cols = [\n#     \"Sex\",\n#     \"IsSeniorCitizen\",\n#     \"HasPartner\",\n#     \"HasChild\",\n#     \"HasPhoneService\",\n#     \"HasMultiplePhoneNumbers\",\n#     \"HasInternetService\",\n#     \"HasOnlineSecurityService\",\n#     \"HasOnlineBackup\",\n#     \"HasDeviceProtection\",\n#     \"HasTechSupportAccess\",\n#     \"HasOnlineTV\",\n#     \"HasMovieSubscription\",\n#     \"HasContractPhone\",\n#     \"IsBillingPaperless\",\n#     \"PaymentMethod\",\n# ]\n\n# target = 'total_target'","metadata":{"execution":{"iopub.status.busy":"2023-11-26T13:38:29.370304Z","iopub.execute_input":"2023-11-26T13:38:29.370985Z","iopub.status.idle":"2023-11-26T13:38:29.376479Z","shell.execute_reply.started":"2023-11-26T13:38:29.370952Z","shell.execute_reply":"2023-11-26T13:38:29.375634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.concat([train_df[num_cols], train_df[target]], axis = 1)\n# train_df","metadata":{"execution":{"iopub.status.busy":"2023-11-26T13:38:29.37766Z","iopub.execute_input":"2023-11-26T13:38:29.377934Z","iopub.status.idle":"2023-11-26T13:38:29.389423Z","shell.execute_reply.started":"2023-11-26T13:38:29.377912Z","shell.execute_reply":"2023-11-26T13:38:29.38857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T13:38:29.390452Z","iopub.execute_input":"2023-11-26T13:38:29.390684Z","iopub.status.idle":"2023-11-26T13:38:29.399296Z","shell.execute_reply.started":"2023-11-26T13:38:29.390663Z","shell.execute_reply":"2023-11-26T13:38:29.398476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"3fb26779","papermill":{"duration":0.086748,"end_time":"2022-10-09T03:34:44.680461","exception":false,"start_time":"2022-10-09T03:34:44.593713","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Choose model","metadata":{}},{"cell_type":"markdown","source":"* Используем choos model из предыдущей работы - расширенный - https://www.kaggle.com/code/akscent/choice-of-boosting\n* Отличный ноутбук с оценкой и ансамблированием моделей - https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n* Далее оптимизаторы гиперпараметр**ов - https://www.kaggle.com/code/akscent/ods-boosting\n* Далее стекинг - https://www.kaggle.com/code/akscent/ods-boosting ; https://alexanderdyakonov.wordpress.com/2017/03/10/c%D1%82%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3-stacking-%D0%B8-%D0%B1%D0%BB%D0%B5%D0%BD%D0%B4%D0%B8%D0%BD%D0%B3-blending/ ; https://github.com/a-milenkin/Competitive_Data_Science/blob/main/notebooks/6.3%20-%20Automatic%20Stacking.ipynb\n* приемы в мл - https://github.com/Dyakonov/ml_hacks/tree/master","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-07T08:00:31.496083Z","iopub.execute_input":"2023-12-07T08:00:31.496935Z","iopub.status.idle":"2023-12-07T08:00:31.507102Z","shell.execute_reply.started":"2023-12-07T08:00:31.496895Z","shell.execute_reply":"2023-12-07T08:00:31.506018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_target_0 = train_df[train_df['total_target'] == 0].head(25000)\n# df_target_1 = train_df[train_df['total_target'] == 1].head(25000)\n\n# train_df = pd.concat([df_target_0, df_target_1], ignore_index=True)\n# train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T13:38:30.456501Z","iopub.execute_input":"2023-11-26T13:38:30.456764Z","iopub.status.idle":"2023-11-26T13:38:30.460655Z","shell.execute_reply.started":"2023-11-26T13:38:30.456741Z","shell.execute_reply":"2023-11-26T13:38:30.459707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One-Hot-Encoder for city type","metadata":{}},{"cell_type":"code","source":"test_df.isnull().sum().max()","metadata":{"execution":{"iopub.status.busy":"2023-11-26T15:49:53.345372Z","iopub.execute_input":"2023-11-26T15:49:53.346116Z","iopub.status.idle":"2023-11-26T15:49:53.436413Z","shell.execute_reply.started":"2023-11-26T15:49:53.346086Z","shell.execute_reply":"2023-11-26T15:49:53.435561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train","metadata":{"execution":{"iopub.status.busy":"2023-11-26T15:52:52.141033Z","iopub.execute_input":"2023-11-26T15:52:52.141932Z","iopub.status.idle":"2023-11-26T15:52:52.145903Z","shell.execute_reply.started":"2023-11-26T15:52:52.141898Z","shell.execute_reply":"2023-11-26T15:52:52.144889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_columns = x_train.select_dtypes(include=['object']).columns\nx_train[object_columns] = x_train[object_columns].astype('category')\ntest_df[object_columns] = test_df[object_columns].astype('category')","metadata":{"execution":{"iopub.status.busy":"2023-11-26T16:05:48.149503Z","iopub.execute_input":"2023-11-26T16:05:48.149932Z","iopub.status.idle":"2023-11-26T16:05:49.348968Z","shell.execute_reply.started":"2023-11-26T16:05:48.149901Z","shell.execute_reply":"2023-11-26T16:05:49.347675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat_cols = [\n#     'channel_code',\n#     'index_city_code', 'ogrn_month', 'ogrn_year',\n#     'branch_code', 'okved', 'segment', 'city_type_1252', 'city_type_3597', 'city_type_other'\n# ]\n# type_city = ['city_type_1252', 'city_type_3597', 'city_type_other']\n\n# cat_cols = list(x_train[object_columns].astype('category').columns)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:36:43.128174Z","iopub.execute_input":"2023-12-07T06:36:43.128903Z","iopub.status.idle":"2023-12-07T06:36:43.132951Z","shell.execute_reply.started":"2023-12-07T06:36:43.128866Z","shell.execute_reply":"2023-12-07T06:36:43.131863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.columns","metadata":{"execution":{"iopub.status.busy":"2023-11-26T16:05:55.410279Z","iopub.execute_input":"2023-11-26T16:05:55.410758Z","iopub.status.idle":"2023-11-26T16:05:55.423486Z","shell.execute_reply.started":"2023-11-26T16:05:55.410715Z","shell.execute_reply":"2023-11-26T16:05:55.422654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-11-26T13:38:30.588574Z","iopub.execute_input":"2023-11-26T13:38:30.588841Z","iopub.status.idle":"2023-11-26T13:38:31.029971Z","shell.execute_reply.started":"2023-11-26T13:38:30.588818Z","shell.execute_reply":"2023-11-26T13:38:31.028958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ndef evaluate_classification_metrics(y_true, y_pred, model_name):\n    cm = confusion_matrix(y_true, y_pred)\n    print(f\"Classification Report for {model_name}:\\n\", classification_report(y_true, y_pred))\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:55:07.505194Z","iopub.execute_input":"2023-12-07T06:55:07.5059Z","iopub.status.idle":"2023-12-07T06:55:07.512492Z","shell.execute_reply.started":"2023-12-07T06:55:07.505864Z","shell.execute_reply":"2023-12-07T06:55:07.511117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_classification_metrics(y_true, y_pred, model_name):\n    \"\"\"\n    By class confusion matrix and report for other multi-class metrices\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    print(f\"Classification Report for {model_name}:\\n\", classification_report(y_true, y_pred))\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom catboost import CatBoostClassifier, Pool\nfrom xgboost import XGBClassifier, DMatrix\nfrom lightgbm import LGBMClassifier, train, Dataset\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\n\n# Optimization Hyperparameters\nimport optuna\nimport torch\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Stacking \nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\n\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-12-07T13:48:28.810828Z","iopub.execute_input":"2023-12-07T13:48:28.81119Z","iopub.status.idle":"2023-12-07T13:48:28.823108Z","shell.execute_reply.started":"2023-12-07T13:48:28.811159Z","shell.execute_reply":"2023-12-07T13:48:28.822259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EvalModels:\n    \"\"\"\n    Class for evaluate binary classification models\n    \"\"\"\n    \n    def __init__(self, data, y_col, test_size=0.2, train_size=0.7, random_state=43):\n        self.data = data\n        self.y_col = y_col\n        self.x_cols = list(data.drop(columns=y_col).columns)\n        self.test_size = test_size\n        self.train_size = train_size\n        self.random_state = random_state\n    \n    def simple_eval(self, cv_splits=5):\n        \"\"\"\n        data - pd.dataFrame\n        y_col - list with target col name\n        cv_split - split for cross validation\n        test_size, train_size  - % of splitting\n        random state - rand state for models\n        \"\"\"\n        \n        object_columns = self.data[self.x_cols].select_dtypes(include=['object']).columns\n        cat_idx = [self.data.columns.get_loc(col) for col in object_columns.to_list()]\n        if not object_columns.empty:\n            self.data[object_columns] = self.data[object_columns].astype('category')\n\n        # list with models. Стоит продолжить расширять.\n        MLA = []\n\n        if not object_columns.empty:\n            MLA.append(XGBClassifier(\n                verbose=-1, enable_categorical=True,\n            ))\n        else:\n            MLA.append(XGBClassifier(\n                verbose=-1,\n                enable_categorical=False,\n            ))\n\n        if not object_columns.empty:\n            MLA.append(LGBMClassifier(\n                verbose=-1, cat_feature=cat_idx, \n                n_estimators=100,\n                learning_rate=0.1,\n                reg_alpha=0.5,\n                reg_lambda=0.3,\n                seed=42,\n            ))\n        else:\n            MLA.append(LGBMClassifier(\n                verbose=-1,\n                n_estimators=100,\n                learning_rate=0.1,\n                reg_alpha=0.5,\n                reg_lambda=0.3,\n                seed=42,\n            ))\n\n        if not object_columns.empty:\n            MLA.append(CatBoostClassifier(\n                verbose=0, cat_features=object_columns,\n                depth=4, iterations=3500, colsample_bylevel=0.098,\n                subsample=0.95, l2_leaf_reg=9, min_data_in_leaf=243,\n                max_bin=187,\n            ))\n        else:\n            MLA.append(CatBoostClassifier(\n                verbose=0, depth=4, iterations=3500,\n                colsample_bylevel=0.098, subsample=0.95,\n                l2_leaf_reg=9, min_data_in_leaf=243,\n                max_bin=187,\n            ))\n\n        MLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Train Accuracy Mean', 'MLA Test Accuracy Mean',\n                       'MLA Test Accuracy 3*STD', 'MLA Time']\n        MLA_compare = pd.DataFrame(columns=MLA_columns)\n\n        cv_split = model_selection.StratifiedShuffleSplit(n_splits=cv_splits, test_size=test_size,\n                                                          train_size=train_size, random_state=random_state)\n\n        row_index = 0\n        for alg in tqdm(MLA, total=len(MLA), desc=\"ML first estimate\"):\n            MLA_name = alg.__class__.__name__\n            MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n            MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n\n            cv_results = model_selection.cross_validate(alg, self.data[self.x_cols], self.data[self.y_col], cv=cv_split,\n                                                        return_train_score=True)\n            print(f\"{MLA_name} estimated: {cv_results['test_score'].mean()}\")\n            MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n            MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n            MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()\n            MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std() * 3\n\n            row_index += 1\n\n        MLA_compare.sort_values(by=['MLA Test Accuracy Mean'], ascending=False, inplace=True)\n        list_top_models = MLA_compare.head(3)['MLA Name'].tolist()\n\n        return MLA_compare, list_top_models\n\n\n#     result, best_list = simple_eval(data, y_col)\n\n    def fit_model(trial, X_train, y_train, X_valid, y_valid, model_name):\n\n        if model_name == \"CatBoostClassifier\":\n            params = {\n                \"iterations\": trial.suggest_int(\"iterations\", 500, 800),\n                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.01),\n                \"auto_class_weights\": trial.suggest_categorical(\"auto_class_weights\", [\"SqrtBalanced\", \"Balanced\", \"None\"]),\n                \"depth\": trial.suggest_int(\"depth\", 3, 6),\n                'loss_function':'Logloss',\n                'use_best_model': True,\n                'nan_mode': trial.suggest_categorical(\"nan_mode\", [\"Min\", \"Max\"])}\n            train_dataset = Pool(data=X_train, label=y_train,)\n            eval_dataset = Pool(data=X_valid, label=y_valid, )\n            clf = CatBoostClassifier(verbose = 0, random_seed = 41, **params)\n            clf.fit(train_dataset, eval_set=eval_dataset, early_stopping_rounds=300)\n        elif model_name == \"LGBMClassifier\":\n            params = {'num_iterations': trial.suggest_int('num_iterations', 300, 500),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n                'num_leaves': trial.suggest_int('num_leaves', 10, 20),\n                'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.5),\n                'boosting_type': trial.suggest_categorical(\"boosting_type\", ['gbdt', 'dart', 'goss']),\n                'objective': 'binary',\n                'metric': 'binary_logloss',\n                'max_bin': trial.suggest_int('max_bin', 255, 4095),\n                'force_col_wise': True,\n                'is_unbalance': True}\n\n            clf = LGBMClassifier(verbose = -1, random_seed = 42)\n            clf = clf.set_params(**params)\n            clf.fit(X_train, y_train)\n        elif model_name == \"XGBClassifier\":\n            params = {\n                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n                'n_estimators': trial.suggest_int('n_estimators', 300, 500),\n                'eta': trial.suggest_float('eta', 0.01, 0.1),\n                'min_child_weight': trial.suggest_float('min_child_weight', 1, 5),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n                'objective': 'binary:logistic',\n                'enable_categorical': True}\n            clf = XGBClassifier(tree_method = 'hist', device= 'cuda') if gpu_available else XGBClassifier(verbose = -1, random_seed = 43)\n            clf = clf.set_params(**params)\n            clf.fit(X_train, y_train)\n        else:\n            raise ValueError(\"Invalid model_name\")\n        y_pred = clf.predict_proba(X_valid)[:, 1]\n        return clf, y_pred\n\n\n    def objective(self, trial, model_name, return_models=False):\n        sss = StratifiedShuffleSplit(n_splits=self.n_split, test_size=self.test_size, random_state=self.random_state)\n        scores, models = [], []\n\n        for train_idx, valid_idx in sss.split(self.data[self.x_cols], self.data[self.y_col]):\n            X_train, X_valid = self.data[self.x_cols].iloc[train_idx, :], self.data[self.x_cols].iloc[valid_idx, :]\n            y_train, y_valid = self.data[self.y_col].iloc[train_idx], self.data[self.y_col].iloc[valid_idx]\n\n            X_train.columns = X_train.columns.str.replace(' ', '_')\n            X_valid.columns = X_valid.columns.str.replace(' ', '_')\n\n            clf, y_pred = self.fit_model(trial, X_train, y_train, X_valid, y_valid, model_name)\n            roc_auc = roc_auc_score(y_valid, y_pred)\n            scores.append(roc_auc)\n            models.append(clf)\n            break\n\n        if return_models:\n            return np.mean(scores), models\n        else:\n            return np.mean(scores)\n\n    def optimize_models(self):\n        _, list_best_models = self.simple_eval()\n\n        optimization_results, best_score = [], []\n        for model_name in tqdm(list_best_models, total=len(list_best_models), desc='Optimizing models'):\n            if model_name in list_best_models:\n                study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=10))\n                study.optimize(lambda trial: self.objective(trial, model_name), n_trials=50, n_jobs=-1, show_progress_bar=True)\n                best_model_params = study.best_params\n                valid_scores, model = self.objective(optuna.trial.FixedTrial(study.best_params), model_name, return_models=True)\n\n                result = {\n                    'model_name': model_name,\n                    'num_trials': len(study.trials),\n                    'best_trial_value': valid_scores,\n                    'best_trial_params': best_model_params,\n                }   \n                optimization_results.append(result)\n                best_score.append(valid_scores)\n                print(f'Result for {model_name}: {valid_scores}')\n            else:\n                continue\n\n\n        print(\"Optimization results:\")\n        for result in optimization_results:\n            print(f\"Model: {result['model_name']}, Num Trials: {result['num_trials']}, Best Trial Value: {result['best_trial_value']}\")\n            print(f\"Best Trial Params: {result['best_trial_params']}\")\n            print(\"\\n\")\n                \n        return optimization_results, best_score\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T13:48:29.477075Z","iopub.execute_input":"2023-12-07T13:48:29.477462Z","iopub.status.idle":"2023-12-07T13:48:29.515591Z","shell.execute_reply.started":"2023-12-07T13:48:29.47743Z","shell.execute_reply":"2023-12-07T13:48:29.514563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ev = EvalModels(data, y_col)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T13:48:30.696855Z","iopub.execute_input":"2023-12-07T13:48:30.697239Z","iopub.status.idle":"2023-12-07T13:48:30.703053Z","shell.execute_reply.started":"2023-12-07T13:48:30.697185Z","shell.execute_reply":"2023-12-07T13:48:30.702093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimization_results, best_score = model_ev.optimize_models()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T13:48:31.501557Z","iopub.execute_input":"2023-12-07T13:48:31.50191Z","iopub.status.idle":"2023-12-07T13:48:31.751254Z","shell.execute_reply.started":"2023-12-07T13:48:31.501881Z","shell.execute_reply":"2023-12-07T13:48:31.749945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.head(3)['MLA Name'].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T08:14:55.80091Z","iopub.execute_input":"2023-12-07T08:14:55.801401Z","iopub.status.idle":"2023-12-07T08:14:55.809512Z","shell.execute_reply.started":"2023-12-07T08:14:55.801365Z","shell.execute_reply":"2023-12-07T08:14:55.808397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result['MLA Parameters']","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:08:49.061897Z","iopub.execute_input":"2023-12-07T10:08:49.062724Z","iopub.status.idle":"2023-12-07T10:08:49.100731Z","shell.execute_reply.started":"2023-12-07T10:08:49.062686Z","shell.execute_reply":"2023-12-07T10:08:49.099466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result[\"MLA Test Accuracy Mean\"] = result[\"MLA Test Accuracy Mean\"].astype(float)\n# list_best_models = list(result.nlargest(3, 'MLA Test Accuracy Mean')[\"MLA Name\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:40:00.642595Z","iopub.execute_input":"2023-12-06T10:40:00.64356Z","iopub.status.idle":"2023-12-06T10:40:00.647215Z","shell.execute_reply.started":"2023-12-06T10:40:00.643523Z","shell.execute_reply":"2023-12-06T10:40:00.646282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_best_models = ['XGBClassifier', 'CatBoostClassifier', 'LGBMClassifier']","metadata":{"execution":{"iopub.status.busy":"2023-12-07T08:26:25.675464Z","iopub.execute_input":"2023-12-07T08:26:25.675898Z","iopub.status.idle":"2023-12-07T08:26:25.680837Z","shell.execute_reply.started":"2023-12-07T08:26:25.675862Z","shell.execute_reply":"2023-12-07T08:26:25.679793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Search hyperparams","metadata":{}},{"cell_type":"code","source":"for result in optimization_results:\n    model_name = result['model_name']\n    best_params = result['best_trial_params']\n\n    if model_name == 'LGBMClassifier':\n        model = LGBMClassifier(**best_params)\n    elif model_name == 'XGBClassifier':\n        model = XGBClassifier(**best_params)\n    elif model_name == 'CatBoostClassifier':\n        model = CatBoostClassifier(**best_params)\n    else:\n        print(f\"Unsupported model: {model_name}\")\n        continue\n\n    # Assuming you have a function train_model() for training the model\n    trained_model = train_model(model, self.data[self.x_cols], self.data[self.y_col])\n\n    # Assuming you have a function evaluate_model() for evaluating the model\n    evaluation_result = evaluate_model(trained_model, self.data[self.x_cols], self.data[self.y_col])\n\n    print(f\"Model: {model_name}, Evaluation Result: {evaluation_result}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.select_dtypes(include=['category']).columns","metadata":{"execution":{"iopub.status.busy":"2023-11-26T13:38:31.175582Z","iopub.execute_input":"2023-11-26T13:38:31.175907Z","iopub.status.idle":"2023-11-26T13:38:31.185448Z","shell.execute_reply.started":"2023-11-26T13:38:31.175873Z","shell.execute_reply":"2023-11-26T13:38:31.184557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train[x_cols]","metadata":{"execution":{"iopub.status.busy":"2023-11-26T13:38:31.188733Z","iopub.execute_input":"2023-11-26T13:38:31.189267Z","iopub.status.idle":"2023-11-26T13:38:31.369358Z","shell.execute_reply.started":"2023-11-26T13:38:31.189228Z","shell.execute_reply":"2023-11-26T13:38:31.36849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#     # Список гиперпараметров для каждой модели\n#     MLA_params = [\n#         {'name': 'AdaBoost', 'params': {'n_estimators': trial.suggest_int('n_estimators', 2000, 5000), 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),}},\n#     #     {'name': 'BaggingClassifier', 'params': {'n_estimators': [10, 50, 100], 'max_samples': [0.5, 0.7, 1.0]}},\n#     #     {'name': 'ExtraTrees', 'params': {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2']}},\n#     #     {'name': 'GradientBoosting', 'params': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}},\n#     #     {'name': 'RandomForest', 'params': {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2']}},\n#     #     {'name': 'LogisticRegressionCV', 'params': {'Cs': [1, 10, 100], 'penalty': ['l1', 'l2']}},\n#     #     {'name': 'PassiveAggressiveClassifier', 'params': {'C': [0.1, 1, 10], 'max_iter': [100, 200, 500]}},\n#     #     {'name': 'RidgeClassifierCV', 'params': {'alphas': [(1e-3, 1e-2, 1e-1), (1, 10, 100)]}},\n#     #     {'name': 'SGDClassifier', 'params': {'alpha': [1e-3, 1e-2, 1e-1], 'max_iter': [100, 200, 500]}},\n#     #     {'name': 'Perceptron', 'params': {'alpha': [1e-3, 1e-2, 1e-1], 'max_iter': [100, 200, 500]}},\n#     #     {'name': 'BernoulliNB', 'params': {'alpha': [0.1, 1, 2], 'binarize': [0.0, 0.5, 1.0]}},\n#     #     {'name': 'GaussianNB', 'params': {}},\n#         {'name': 'KNeighborsClassifier', 'params': {'n_neighbors': trial.suggest_int('n_neighbors',3, 7), \n#                                                     'weights': trial.suggest_categorical(\"weights\", ['uniform', 'distance'])}},\n#     #     {'name': 'SVC', 'params': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}},\n#     #     {'name': 'NuSVC', 'params': {'nu': [0.1, 0.3, 0.5], 'kernel': ['linear', 'rbf']}},\n#     #     {'name': 'LinearSVC', 'params': {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}},\n#     #     {'name': 'DecisionTreeClassifier', 'params': {'criterion': ['gini', 'entropy'], 'max_depth': [3, 5, 7]}},\n#         {'name': 'ExtraTreesClassifier', 'params': {'criterion': trial.suggest_categorical(\"criterion\", ['gini', 'entropy']), \n#                                                    'max_depth': trial.suggest_categorical('max_depth', [3, 7])}},\n\n#     #     {'name': 'LinearDiscriminantAnalysis', 'params': {}},\n#     #     {'name': 'QuadraticDiscriminantAnalysis', 'params': {}},\n#         {'name': 'XGBClassifier', 'params': {\n#             'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n#             'n_estimators': trial.suggest_int('n_estimators', 300, 500),\n#             'eta': trial.suggest_float('eta', 0.01, 0.1),\n#             'booster': trial.suggest_categorical(\"booster\", ['gbdt', 'dart', 'goss']),\n#             'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),\n#             'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n#             'objective': 'binary:logistic',}},\n#         {'name': 'CatBoostClassifier', 'params': {\n#             \"iterations\": trial.suggest_int(\"iterations\", 500, 800),\n#             \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.01),\n#             \"auto_class_weights\": trial.suggest_categorical(\"auto_class_weights\", [\"SqrtBalanced\", \"Balanced\", \"None\"]),\n#             \"depth\": trial.suggest_int(\"depth\", 3, 6),\n#             'loss_function':'Logloss',\n#             'use_best_model': True,\n#             'nan_mode': trial.suggest_categorical(\"nan_mode\", [\"Min\", \"Max\"]),\n#         }},\n#         {'name': 'LGBMClassifier', 'params': {'num_iterations': trial.suggest_int('num_iterations', 300, 500),\n#             'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n#             'num_leaves': trial.suggest_int('num_leaves', 10, 20),\n#             'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.5),\n#             'boosting_type': trial.suggest_categorical(\"boosting_type\", ['gbdt', 'dart', 'goss']),\n#             'objective': 'binary',\n#             'metric': 'binary_logloss',\n#             'force_col_wise': True,\n#             'is_unbalance': True}}\n#     ]\n    \n#     available_model_names = [model['name'] for model in MLA_params]\n#     if model_name not in available_model_names:\n#         print(f\"Error: Model name '{model_name}' not found in available models. Please check the model name.\")\n#     else:\n#         filtered_MLA_params = None\n#         for model in MLA_params:\n#             if model['name'] == model_name:\n#                 filtered_MLA_params = model['params']\n#                 break\n#             else:\n#                 continue\n\n#         if not filtered_MLA_params:\n#             print(f\"Warning: No parameters found for the specified model '{model_name}'.\")\n#         else:\n#             if not filtered_MLA_params:\n#                 print(f\"Model {model_name} not found in MLA_params. Skipping optimization.\")\n#                 return 0","metadata":{"execution":{"iopub.status.busy":"2023-11-26T13:38:31.37088Z","iopub.execute_input":"2023-11-26T13:38:31.371255Z","iopub.status.idle":"2023-11-26T13:38:31.379651Z","shell.execute_reply.started":"2023-11-26T13:38:31.371221Z","shell.execute_reply":"2023-11-26T13:38:31.378704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport torch\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom tqdm import tqdm\n\n\n# column_numbers = [5, 6, 9, 10, 11, 16, 17, 88, 89, 90]\n# x_train[type_city] = x_train[type_city].astype('str')\n# test_df[type_city] = test_df[type_city].astype('str')\n# for col in cat_cols:\n#     x_train[col] = pd.Categorical(x_train[col])\n#     test_df[col] = pd.Categorical(test_df[col])\n            \ndef fit_MLA(trial, X_train, y_train, X_valid, y_valid, MLA, model_name):\n            \n    if model_name == \"CatBoostClassifier\":\n        params = {\n            \"iterations\": trial.suggest_int(\"iterations\", 500, 800),\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.01),\n            \"auto_class_weights\": trial.suggest_categorical(\"auto_class_weights\", [\"SqrtBalanced\", \"Balanced\", \"None\"]),\n            \"depth\": trial.suggest_int(\"depth\", 3, 6),\n            'loss_function':'Logloss',\n            'use_best_model': True,\n            'nan_mode': trial.suggest_categorical(\"nan_mode\", [\"Min\", \"Max\"])}\n        train_dataset = Pool(data=X_train, label=y_train,)\n        eval_dataset = Pool(data=X_valid, label=y_valid, )\n        clf = CatBoostClassifier(verbose = 0, random_seed = 41, **params)\n        clf.fit(train_dataset, eval_set=eval_dataset, early_stopping_rounds=300)\n    elif model_name == \"LGBMClassifier\":\n        params = {'num_iterations': trial.suggest_int('num_iterations', 300, 500),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n            'num_leaves': trial.suggest_int('num_leaves', 10, 20),\n            'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.5),\n            'boosting_type': trial.suggest_categorical(\"boosting_type\", ['gbdt', 'dart', 'goss']),\n            'objective': 'binary',\n            'metric': 'binary_logloss',\n            'max_bin': trial.suggest_int('max_bin', 255, 4095),\n            'force_col_wise': True,\n            'is_unbalance': True}\n        \n        clf = LGBMClassifier(verbose = -1, random_seed = 42)\n        clf = clf.set_params(**params)\n        clf.fit(X_train, y_train)\n    elif model_name == \"XGBClassifier\":\n        params = {\n            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n            'n_estimators': trial.suggest_int('n_estimators', 300, 500),\n            'eta': trial.suggest_float('eta', 0.01, 0.1),\n            'min_child_weight': trial.suggest_float('min_child_weight', 1, 5),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n            'objective': 'binary:logistic',\n            'enable_categorical': True}\n        clf = XGBClassifier(tree_method = 'hist', device= 'cuda') if gpu_available else XGBClassifier(verbose = -1, random_seed = 43)\n        clf = clf.set_params(**params)\n        clf.fit(X_train, y_train)\n    else:\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n            'max_depth': trial.suggest_int('max_depth', 3, 20),\n            'min_samples_split': trial.suggest_float('min_samples_split', 0.1, 1.0),\n            'min_samples_leaf': trial.suggest_float('min_samples_leaf', 0.1, 0.5),\n            'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']),}\n        clf = MLA\n        clf = clf.set_params( **params)\n        clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_valid)[:, 1]\n    return clf, y_pred\n\n# Определение функции objective\ndef objective(trial, return_models=False):\n    \n    n_splits = 5\n    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n    scores, models = [], []\n\n    for train_idx, valid_idx in sss.split(data[x_cols], data[y_col]):\n        X_train, X_valid = data[x_cols].iloc[train_idx, :], data[x_cols].iloc[valid_idx, :]\n        y_train, y_valid = data[y_col].iloc[train_idx], data[y_col].iloc[valid_idx]\n\n        X_train.columns = X_train.columns.str.replace(' ', '_')\n        X_valid.columns = X_valid.columns.str.replace(' ', '_')\n\n        clf, y_pred = fit_MLA(trial, X_train, y_train, X_valid, y_valid, model, model_name)\n        roc_auc = roc_auc_score(y_valid, y_pred)\n        scores.append(roc_auc)\n        models.append(clf)\n        break\n        \n    if return_models:\n        return np.mean(scores), models\n    else:\n        return np.mean(scores)\n\n\ndef optim(objective, list_best_models):\n    # Список для сохранения результатов оптимизации по моделям\n    optimization_results, optimization_models = [], []\n    # Оптимизация гиперпараметров для каждой модели\n    for model in tqdm(list_best_models, total=len(list_best_models), desc='Optimizing models'):\n        model_name = model\n        if model_name in list_best_models:\n        #     result, clm = optimize_model(x_train[x_cols], x_train[y_cols], model_name, model)\n            study = optuna.create_study(direction='maximize', sampler=TPESampler())\n        #     study.optimize(lambda trial: objective(trial, X, y, MLA, model_name), n_trials=5)\n            study.optimize(objective, n_trials=50, n_jobs = -1, show_progress_bar=True,)\n            best_model_params = study.best_params\n            model, valid_scores = objective(optuna.trial.FixedTrial(study.best_params), return_models=True)\n\n            result = {\n                'model_name': model_name,\n                'num_trials': len(study.trials),\n                'best_trial_value': valid_scores,\n                'best_trial_params': best_model_params,\n            }   \n            optimization_results.append(result)\n            optimization_models.append(model)\n            print(f'Result for {model_name}: {valid_scores}')\n        else:\n            continue\n            \n    return optimization_results, optimization_models\n\noptimization_results, optimization_models = optim(objective, list_best_models)\nprint(\"Optimization results:\")\nfor result in optimization_results:\n    print(f\"Model: {result['model_name']}, Num Trials: {result['num_trials']}, Best Trial Value: {result['best_trial_value']}\")\n    print(f\"Best Trial Params: {result['best_trial_params']}\")\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T08:34:30.81419Z","iopub.execute_input":"2023-12-07T08:34:30.814783Z","iopub.status.idle":"2023-12-07T08:36:39.05448Z","shell.execute_reply.started":"2023-12-07T08:34:30.814732Z","shell.execute_reply":"2023-12-07T08:36:39.053436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimization_results, optimization_models","metadata":{"execution":{"iopub.status.busy":"2023-12-07T08:54:00.411389Z","iopub.execute_input":"2023-12-07T08:54:00.411787Z","iopub.status.idle":"2023-12-07T08:54:00.417821Z","shell.execute_reply.started":"2023-12-07T08:54:00.411755Z","shell.execute_reply":"2023-12-07T08:54:00.41685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_models = []  # Список для хранения обученных моделей\n\nfor result in optimization_results:\n    for model in MLA:\n        if model.__class__.__name__ == result['model_name']:\n            valid_params = model.get_params()\n            filtered_params = {k: v for k, v in result['best_trial_params'].items() if k in valid_params}\n            \n            # Создаем новый экземпляр модели с лучшими гиперпараметрами\n            if result['model_name'] == \"CatBoostClassifier\":\n                model_instance = CatBoostClassifier(**filtered_params)\n            else:\n                model_instance = model.set_params(**filtered_params)\n            \n            trained_models.append(model_instance)\n            ","metadata":{"execution":{"iopub.status.busy":"2023-12-06T11:47:07.520057Z","iopub.execute_input":"2023-12-06T11:47:07.520446Z","iopub.status.idle":"2023-12-06T11:47:07.527932Z","shell.execute_reply.started":"2023-12-06T11:47:07.520413Z","shell.execute_reply":"2023-12-06T11:47:07.526918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_models","metadata":{"execution":{"iopub.status.busy":"2023-12-06T11:47:22.517903Z","iopub.execute_input":"2023-12-06T11:47:22.518689Z","iopub.status.idle":"2023-12-06T11:47:22.526238Z","shell.execute_reply.started":"2023-12-06T11:47:22.518642Z","shell.execute_reply":"2023-12-06T11:47:22.525321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\ndef print_roca_auc(clf, X, y, label):\n    a = clf.predict(X)\n    \n    print(label + ' AUC-ROC  = ' + str(roc_auc_score(y, a)))","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:52:08.342278Z","iopub.execute_input":"2023-12-06T10:52:08.342664Z","iopub.status.idle":"2023-12-06T10:52:08.347693Z","shell.execute_reply.started":"2023-12-06T10:52:08.342636Z","shell.execute_reply":"2023-12-06T10:52:08.346709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\n\nclass DjStacking(BaseEstimator, ClassifierMixin):  \n    \"\"\"Стэкинг моделей scikit-learn\"\"\"\n\n    def __init__(self, models, ens_model):\n        \"\"\"\n        Инициализация\n        models - базовые модели для стекинга\n        ens_model - мета-модель\n        \"\"\"\n        self.models = models\n        self.ens_model = ens_model\n        self.n = len(models)\n        self.valid = None\n        \n    def fit(self, X, y=None, p=0.25, cv=5, err=0.001, random_state=None):\n        \"\"\"\n        Обучение стекинга\n        p - в каком отношении делить на обучение / тест\n            если p = 0 - используем всё обучение!\n        cv  (при p=0) - сколько фолдов использовать\n        err (при p=0) - величина случайной добавки к метапризнакам\n        random_state - инициализация генератора\n            \n        \"\"\"\n        if (p > 0): # делим на обучение и тест\n            # разбиение на обучение моделей и метамодели\n            train, valid, y_train, y_valid = train_test_split(X, y, test_size=p, random_state=random_state)\n            \n            # заполнение матрицы для обучения метамодели\n            self.valid = np.zeros((valid.shape[0], self.n))\n            for t, clf in enumerate(self.models):\n                clf.fit(train, y_train)\n                self.valid[:, t] = clf.predict_proba(valid)[:,1]\n                \n            # обучение метамодели\n            self.ens_model.fit(self.valid, y_valid)\n            \n        else: # используем всё обучение\n            \n            # для регуляризации - берём случайные добавки\n            self.valid = err*np.random.randn(X.shape[0], self.n)\n            \n            for t, clf in enumerate(self.models):\n                # это oob-ответы алгоритмов\n                self.valid[:, t] += cross_val_predict(clf, X, y, cv=cv, n_jobs=-1, method='predict')\n                # но сам алгоритм надо настроить\n                clf.fit(X, y)\n            \n            # обучение метамодели\n            self.ens_model.fit(self.valid, y)  \n            \n\n        return self\n    \n\n\n    def predict(self, X, y=None):\n        \"\"\"\n        Работа стэкинга\n        \"\"\"\n        # заполение матрицы для мета-классификатора\n        X_meta = np.zeros((X.shape[0], self.n))\n        \n        for t, clf in enumerate(self.models):\n            X_meta[:, t] = clf.predict_proba(X)[:, 1]\n        \n        a = self.ens_model.predict_proba(X_meta)[:, 1]\n        \n        return (a)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:52:19.291515Z","iopub.execute_input":"2023-12-06T10:52:19.291876Z","iopub.status.idle":"2023-12-06T10:52:19.305918Z","shell.execute_reply.started":"2023-12-06T10:52:19.291847Z","shell.execute_reply":"2023-12-06T10:52:19.305086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\ntrain_X, test_X, train_y, test_y = train_test_split(x_train, y, train_size=0.8, \n                                                    random_state=1999, stratify=x_train[y_cols])\nens_model = Ridge()\ns1 = DjStacking(trained_models, ens_model)\ns1.fit(train_X, train_y)\n\n# model_name = f'{run.name}_model.mod'\n# bstr = s1.booster_\n# bstr.save_model(model_name)\n# config = s1.get_params()\n# model_art = wandb.Artifact(name=model_name, type='model', metadata=config)\n# model_art.add_file(model_name)\n# run.log_artifact(model_art)\n\n# run.summary[\"best_score\"] = bstr.best_score\n# run.summary[\"best_iteration\"] = bstr.best_iteration\n# preds = model.predict(x_val)\n# run.summary[\"f1_score\"] = f1_score(y_val, preds, average='macro')\n# run.summary[\"accuracy\"] = accuracy_score(y_val, preds)\n\n# model_name = f'{run.name}_model.mod'\n# bstr = s2.booster_\n# bstr.save_model(model_name)\n# config = s2.get_params()\n# model_art = wandb.Artifact(name=model_name, type='model', metadata=config)\n# model_art.add_file(model_name)\n# run.log_artifact(model_art)\n\n# run.summary[\"best_score\"] = bstr.best_score\n# run.summary[\"best_iteration\"] = bstr.best_iteration\n# preds = model.predict(x_val)\n# run.summary[\"f1_score\"] = f1_score(y_val, preds, average='macro')\n# run.summary[\"accuracy\"] = accuracy_score(y_val, preds)\n\n# run.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_roca_auc(s1, test_X, test_y, '1-stacking')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s1.predict(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_df = test_df.astype({'channel_code': 'category', 'branch_code': 'category'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# s1 = DjStacking(trained_models, ens_model)\n# a = 0\n# e = []\n# for t in range(2, 11):\n#     s1.fit(train_X, train_y, p=-1, cv=t, err=0.00)\n#     a = s1.predict(test_X, train_y)\n#     auc = roc_auc_score(test_y, a)\n#     print (auc)\n#     e.append(auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_df = pd.read_csv(\"/kaggle/input/sibalfahack/Siberian Alfa Hack Materials/Siberian Alfa Hack Materials/sample_submission.csv\")\n\nfor i in trained_models:\n    if i.__class__.__name__ == CatBoostClassifier:\n        \n        score = i.predict_proba(test_df)[:, 1]\n        sample_submission_df[\"score\"] = score\n        sample_submission_df.to_csv(f\"my_submission_{i}.csv\", index=False)\n    else:\n        clf = i\n        clf.fit(x_train[x_cols], x_train[y_cols])\n        score = i.predict_proba(test_df)[:, 1]\n        sample_submission_df[\"score\"] = score\n        sample_submission_df.to_csv(f\"my_submission_{i}.csv\", index=False)\n    \n# score = s1.predict(test_df)\n# sample_submission_df[\"score\"] = score\n# sample_submission_df.to_csv(\"my_submission_essemble.csv\", index=False)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-26T16:55:17.589069Z","iopub.execute_input":"2023-11-26T16:55:17.58995Z","iopub.status.idle":"2023-11-26T16:55:21.438221Z","shell.execute_reply.started":"2023-11-26T16:55:17.589918Z","shell.execute_reply":"2023-11-26T16:55:21.436867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline for ensemble","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}